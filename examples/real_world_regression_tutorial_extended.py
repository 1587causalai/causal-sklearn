#!/usr/bin/env python3
"""
ğŸ  æ‰©å±•ç‰ˆçœŸå®ä¸–ç•Œå›å½’æ•™ç¨‹ï¼šåŠ å·æˆ¿ä»·é¢„æµ‹
==========================================

è¿™ä¸ªæ•™ç¨‹æ¼”ç¤ºCausalEngineä¸å¤šç§å¼ºåŠ›ä¼ ç»Ÿæ–¹æ³•åœ¨çœŸå®ä¸–ç•Œå›å½’ä»»åŠ¡ä¸­çš„æ€§èƒ½å¯¹æ¯”ã€‚

æ•°æ®é›†ï¼šåŠ å·æˆ¿ä»·æ•°æ®é›†ï¼ˆCalifornia Housing Datasetï¼‰
- 20,640ä¸ªæ ·æœ¬
- 8ä¸ªç‰¹å¾ï¼ˆæˆ¿å±‹å¹´é¾„ã€æ”¶å…¥ã€äººå£ç­‰ï¼‰
- ç›®æ ‡ï¼šé¢„æµ‹æˆ¿ä»·ä¸­ä½æ•°

æˆ‘ä»¬å°†æ¯”è¾ƒ12ç§æ–¹æ³•ï¼š
1. sklearn MLPRegressorï¼ˆä¼ ç»Ÿç¥ç»ç½‘ç»œï¼‰
2. PyTorch MLPï¼ˆä¼ ç»Ÿæ·±åº¦å­¦ä¹ ï¼‰
3. MLP Huberï¼ˆHuberæŸå¤±ç¨³å¥å›å½’ï¼‰
4. MLP Pinballï¼ˆPinballæŸå¤±ç¨³å¥å›å½’ï¼‰
5. MLP Cauchyï¼ˆCauchyæŸå¤±ç¨³å¥å›å½’ï¼‰
6. Random Forestï¼ˆéšæœºæ£®æ—ï¼‰
7. XGBoostï¼ˆæ¢¯åº¦æå‡ï¼‰
8. LightGBMï¼ˆè½»é‡æ¢¯åº¦æå‡ï¼‰
9. CatBoostï¼ˆå¼ºåŠ›æ¢¯åº¦æå‡ï¼‰
10. CausalEngine - exogenousï¼ˆå¤–ç”Ÿå™ªå£°ä¸»å¯¼ï¼‰
11. CausalEngine - endogenousï¼ˆå†…ç”Ÿä¸ç¡®å®šæ€§ä¸»å¯¼ï¼‰
12. CausalEngine - standardï¼ˆå†…ç”Ÿ+å¤–ç”Ÿæ··åˆï¼‰

å…³é”®äº®ç‚¹ï¼š
- çœŸå®ä¸–ç•Œæ•°æ®çš„é²æ£’æ€§æµ‹è¯•
- 6ç§å¼ºåŠ›ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•å¯¹æ¯”
- 3ç§ç¨³å¥ç¥ç»ç½‘ç»œå›å½’æ–¹æ³•ï¼ˆHuberã€Pinballã€Cauchyï¼‰
- ç»Ÿä¸€ç¥ç»ç½‘ç»œå‚æ•°é…ç½®ç¡®ä¿å…¬å¹³æ¯”è¾ƒ
- å› æœæ¨ç†vsä¼ ç»Ÿæ–¹æ³•çš„æ€§èƒ½å·®å¼‚åˆ†æ

å®éªŒè®¾è®¡è¯´æ˜
==================================================================
æœ¬è„šæœ¬åŒ…å«ä¸¤ç»„æ ¸å¿ƒå®éªŒï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°CausalEngineåœ¨çœŸå®å›å½’ä»»åŠ¡ä¸Šçš„æ€§èƒ½å’Œé²æ£’æ€§ã€‚
æ‰€æœ‰å®éªŒå‚æ•°å‡å¯åœ¨ä¸‹æ–¹çš„ `TutorialConfig` ç±»ä¸­è¿›è¡Œä¿®æ”¹ã€‚

å®éªŒä¸€ï¼šæ ¸å¿ƒæ€§èƒ½å¯¹æ¯” (åœ¨40%æ ‡ç­¾å™ªå£°ä¸‹)
--------------------------------------------------
- **ç›®æ ‡**: æ¯”è¾ƒCausalEngineå’Œ9ç§ä¼ ç»Ÿæ–¹æ³•åœ¨å«æœ‰å›ºå®šå™ªå£°æ•°æ®ä¸Šçš„é¢„æµ‹æ€§èƒ½ã€‚
- **è®¾ç½®**: é»˜è®¤è®¾ç½®40%çš„æ ‡ç­¾å™ªå£°ï¼ˆ`ANOMALY_RATIO = 0.4`ï¼‰ï¼Œæ¨¡æ‹ŸçœŸå®ä¸–ç•Œä¸­å¸¸è§çš„æ•°æ®è´¨é‡é—®é¢˜ã€‚
- **å¯¹æ¯”æ¨¡å‹**: 
  - ä¼ ç»Ÿæ–¹æ³•: sklearn MLP, PyTorch MLP, MLP Huber, MLP Pinball, MLP Cauchy, Random Forest, XGBoost, LightGBM, CatBoost
  - CausalEngine: exogenous, endogenous, standardç­‰æ¨¡å¼

å®éªŒäºŒï¼šé²æ£’æ€§åˆ†æ (è·¨è¶Šä¸åŒå™ªå£°æ°´å¹³)
--------------------------------------------------
- **ç›®æ ‡**: æ¢ç©¶æ¨¡å‹æ€§èƒ½éšæ ‡ç­¾å™ªå£°æ°´å¹³å¢åŠ æ—¶çš„å˜åŒ–æƒ…å†µï¼Œè¯„ä¼°å…¶ç¨³å®šæ€§ã€‚
- **è®¾ç½®**: åœ¨ä¸€ç³»åˆ—å™ªå£°æ¯”ä¾‹ï¼ˆå¦‚0%, 10%, 20%, 30%, 40%, 50%ï¼‰ä¸‹åˆ†åˆ«è¿è¡Œæµ‹è¯•ã€‚
- **å¯¹æ¯”æ¨¡å‹**: æ‰€æœ‰12ç§æ–¹æ³•åœ¨ä¸åŒå™ªå£°æ°´å¹³ä¸‹çš„è¡¨ç°
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import fetch_california_housing
import warnings
import os
import sys

# è®¾ç½®matplotlibåç«¯ä¸ºéäº¤äº’å¼ï¼Œé¿å…å¼¹å‡ºçª—å£
plt.switch_backend('Agg')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æ¨¡å—
from causal_sklearn.benchmarks import BaselineBenchmark

warnings.filterwarnings('ignore')


class TutorialConfig:
    """
    æ‰©å±•æ•™ç¨‹é…ç½®ç±» - æ–¹ä¾¿è°ƒæ•´å„ç§å‚æ•°
    
    ğŸ”§ åœ¨è¿™é‡Œä¿®æ”¹å‚æ•°æ¥è‡ªå®šä¹‰å®éªŒè®¾ç½®ï¼
    """
    
    # ğŸ¯ æ•°æ®åˆ†å‰²å‚æ•°
    TEST_SIZE = 0.2          # æµ‹è¯•é›†æ¯”ä¾‹
    VAL_SIZE = 0.2           # éªŒè¯é›†æ¯”ä¾‹ (ç›¸å¯¹äºè®­ç»ƒé›†)
    RANDOM_STATE = 42        # éšæœºç§å­
    
    # ğŸ§  ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½® - æ‰€æœ‰ç¥ç»ç½‘ç»œæ–¹æ³•ä½¿ç”¨ç›¸åŒå‚æ•°ç¡®ä¿å…¬å¹³æ¯”è¾ƒ
    # =========================================================================
    # ğŸ”§ åœ¨è¿™é‡Œä¿®æ”¹æ‰€æœ‰ç¥ç»ç½‘ç»œæ–¹æ³•çš„å…±åŒå‚æ•°ï¼
    NN_HIDDEN_SIZES = (128, 64, 32)                 # ç¥ç»ç½‘ç»œéšè—å±‚ç»“æ„
    NN_MAX_EPOCHS = 3000                            # æœ€å¤§è®­ç»ƒè½®æ•°
    NN_LEARNING_RATE = 0.01                         # å­¦ä¹ ç‡
    NN_PATIENCE = 200                               # æ—©åœpatience
    NN_TOLERANCE = 1e-4                             # æ—©åœtolerance
    # =========================================================================
    
    # ğŸ¤– CausalEngineå‚æ•° - ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    CAUSAL_MODES = ['deterministic', 'exogenous', 'endogenous', 'standard']       # å¯é€‰: ['deterministic', 'exogenous', 'endogenous', 'standard']
    CAUSAL_HIDDEN_SIZES = NN_HIDDEN_SIZES          # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    CAUSAL_MAX_EPOCHS = NN_MAX_EPOCHS               # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    CAUSAL_LR = NN_LEARNING_RATE                    # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    CAUSAL_PATIENCE = NN_PATIENCE                   # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    CAUSAL_TOL = NN_TOLERANCE                       # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    CAUSAL_GAMMA_INIT = 1.0                         # gammaåˆå§‹åŒ–
    CAUSAL_B_NOISE_INIT = 1.0                       # b_noiseåˆå§‹åŒ–
    CAUSAL_B_NOISE_TRAINABLE = True                 # b_noiseæ˜¯å¦å¯è®­ç»ƒ
    
    # ğŸ§  ä¼ ç»Ÿç¥ç»ç½‘ç»œæ–¹æ³•å‚æ•° - ä½¿ç”¨ç»Ÿä¸€é…ç½®
    SKLEARN_HIDDEN_LAYERS = NN_HIDDEN_SIZES         # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    SKLEARN_MAX_ITER = NN_MAX_EPOCHS                # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    SKLEARN_LR = NN_LEARNING_RATE                   # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    
    PYTORCH_EPOCHS = NN_MAX_EPOCHS                  # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    PYTORCH_LR = NN_LEARNING_RATE                   # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    PYTORCH_PATIENCE = NN_PATIENCE                  # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    
    # ğŸ¯ åŸºå‡†æ–¹æ³•é…ç½® - æ‰©å±•ç‰ˆåŒ…å«æ›´å¤šå¼ºåŠ›æ–¹æ³•
    BASELINE_METHODS = [
        'sklearn_mlp',       # sklearnç¥ç»ç½‘ç»œ  
        'pytorch_mlp',       # PyTorchç¥ç»ç½‘ç»œ
        'mlp_huber',         # HuberæŸå¤±MLPï¼ˆç¨³å¥å›å½’ï¼‰
        'mlp_pinball_median',# PinballæŸå¤±MLPï¼ˆç¨³å¥å›å½’ï¼‰
        'mlp_cauchy',        # CauchyæŸå¤±MLPï¼ˆç¨³å¥å›å½’ï¼‰
        'random_forest',     # éšæœºæ£®æ—
        'xgboost',           # XGBoost - å¼ºåŠ›æ¢¯åº¦æå‡
        'lightgbm',          # LightGBM - è½»é‡æ¢¯åº¦æå‡
        'catboost'           # CatBoost - å¼ºåŠ›æ¢¯åº¦æå‡
    ]
    
    # ğŸ“Š å®éªŒå‚æ•°
    ANOMALY_RATIO = 0.4                             # æ ‡ç­¾å¼‚å¸¸æ¯”ä¾‹ (æ ¸å¿ƒå®éªŒé»˜è®¤å€¼: 40%å™ªå£°æŒ‘æˆ˜)
    SAVE_PLOTS = True                               # æ˜¯å¦ä¿å­˜å›¾è¡¨
    VERBOSE = True                                  # æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
    
    # ğŸ›¡ï¸ é²æ£’æ€§æµ‹è¯•å‚æ•° - éªŒè¯"CausalEngineé²æ£’æ€§ä¼˜åŠ¿"çš„å‡è®¾
    ROBUSTNESS_ANOMALY_RATIOS = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]  # å™ªå£°æ°´å¹³
    RUN_ROBUSTNESS_TEST = True                      # æ˜¯å¦è¿è¡Œé²æ£’æ€§æµ‹è¯•
    
    # ğŸ“ˆ å¯è§†åŒ–å‚æ•°
    FIGURE_DPI = 300                                # å›¾è¡¨åˆ†è¾¨ç‡
    FIGURE_SIZE_ANALYSIS = (24, 20)                 # æ•°æ®åˆ†æå›¾è¡¨å¤§å°
    FIGURE_SIZE_PERFORMANCE = (24, 20)              # æ€§èƒ½å¯¹æ¯”å›¾è¡¨å¤§å°
    FIGURE_SIZE_ROBUSTNESS = (24, 20)               # é²æ£’æ€§æµ‹è¯•å›¾è¡¨å¤§å° (æ›´å¤§å®¹çº³æ›´å¤šæ–¹æ³•)
    
    # ğŸ“ è¾“å‡ºç›®å½•å‚æ•°
    OUTPUT_DIR = "results/california_housing_regression_extended"  # è¾“å‡ºç›®å½•åç§°


class ExtendedCaliforniaHousingTutorial:
    """
    æ‰©å±•ç‰ˆåŠ å·æˆ¿ä»·å›å½’æ•™ç¨‹ä¸»ç±»
    
    å®ç°äº†çœŸå®ä¸–ç•Œæ•°æ®ä¸Šçš„å…¨é¢æ€§èƒ½è¯„ä¼°ï¼ŒåŒ…å«å¤šç§å¼ºåŠ›ä¼ ç»Ÿæ–¹æ³•å¯¹æ¯”
    """
    
    def __init__(self, config=None):
        self.config = config or TutorialConfig()
        self.benchmark = BaselineBenchmark()
        self.results = {}
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(self.config.OUTPUT_DIR, exist_ok=True)
        
        if self.config.VERBOSE:
            print("ğŸ  æ‰©å±•ç‰ˆåŠ å·æˆ¿ä»·å›å½’æ•™ç¨‹")
            print("=" * 60)
            print(f"ğŸ“Š å°†æ¯”è¾ƒ {len(self.config.BASELINE_METHODS) + len(self.config.CAUSAL_MODES)} ç§æ–¹æ³•")
            print(f"ğŸ¯ åŸºå‡†æ–¹æ³• ({len(self.config.BASELINE_METHODS)}ç§): {', '.join(self.config.BASELINE_METHODS)}")
            print(f"ğŸ¤– CausalEngineæ¨¡å¼ ({len(self.config.CAUSAL_MODES)}ç§): {', '.join(self.config.CAUSAL_MODES)}")
            print(f"ğŸ“ ç»“æœä¿å­˜åˆ°: {self.config.OUTPUT_DIR}")
            print()
    
    def load_and_analyze_data(self):
        """åŠ è½½å¹¶åˆ†æåŠ å·æˆ¿ä»·æ•°æ®é›†"""
        if self.config.VERBOSE:
            print("ğŸ“¥ åŠ è½½åŠ å·æˆ¿ä»·æ•°æ®é›†...")
        
        # åŠ è½½æ•°æ®
        california_housing = fetch_california_housing()
        X = california_housing.data
        y = california_housing.target
        feature_names = california_housing.feature_names
        
        if self.config.VERBOSE:
            print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ:")
            print(f"   - æ ·æœ¬æ•°: {X.shape[0]:,}")
            print(f"   - ç‰¹å¾æ•°: {X.shape[1]}")
            print(f"   - ç‰¹å¾å: {', '.join(feature_names)}")
            print(f"   - æˆ¿ä»·èŒƒå›´: ${y.min():.2f} - ${y.max():.2f} (å•ä½: 10ä¸‡ç¾å…ƒ)")
            print(f"   - æˆ¿ä»·å‡å€¼: ${y.mean():.2f} Â± ${y.std():.2f}")
            print()
        
        # ä¿å­˜æ•°æ®ä¿¡æ¯ä¾›åç»­ä½¿ç”¨
        self.X = X
        self.y = y
        self.feature_names = feature_names
        
        # ç”Ÿæˆæ•°æ®åˆ†æå›¾è¡¨
        if self.config.SAVE_PLOTS:
            self._create_data_analysis_plots()
        
        return X, y, feature_names
    
    def _create_data_analysis_plots(self):
        """åˆ›å»ºæ•°æ®åˆ†æå›¾è¡¨"""
        if self.config.VERBOSE:
            print("ğŸ“Š ç”Ÿæˆæ•°æ®åˆ†æå›¾è¡¨...")
        
        # åˆ›å»ºç»¼åˆæ•°æ®åˆ†æå›¾
        fig, axes = plt.subplots(2, 2, figsize=self.config.FIGURE_SIZE_ANALYSIS, dpi=self.config.FIGURE_DPI)
        fig.suptitle('California Housing Dataset Analysis - Extended Regression Tutorial', fontsize=16, fontweight='bold')
        
        # 1. æˆ¿ä»·åˆ†å¸ƒ
        axes[0,0].hist(self.y, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
        axes[0,0].axvline(self.y.mean(), color='red', linestyle='--', alpha=0.8, 
                         label=f'Mean: ${self.y.mean():.2f}')
        axes[0,0].set_xlabel('House Price ($100k)')
        axes[0,0].set_ylabel('Frequency')
        axes[0,0].set_title('House Price Distribution')
        axes[0,0].legend()
        axes[0,0].grid(True, alpha=0.3)
        
        # 2. ç‰¹å¾ç›¸å…³çŸ©é˜µ
        feature_data = pd.DataFrame(self.X, columns=self.feature_names)
        feature_data['MedHouseVal'] = self.y
        corr_matrix = feature_data.corr()
        
        sns.heatmap(corr_matrix, annot=True, cmap='RdBu_r', center=0, 
                   square=True, ax=axes[0,1], cbar_kws={'shrink': 0.8})
        axes[0,1].set_title('Feature Correlation Matrix')
        
        # 3. ç‰¹å¾åˆ†å¸ƒï¼ˆæ ‡å‡†åŒ–åï¼‰
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(self.X)
        feature_data_scaled = pd.DataFrame(X_scaled, columns=self.feature_names)
        
        feature_data_scaled.boxplot(ax=axes[1,0])
        axes[1,0].set_title('Feature Distribution (Standardized)')
        axes[1,0].set_ylabel('Standardized Value')
        axes[1,0].tick_params(axis='x', rotation=45)
        
        # 4. æœ€ç›¸å…³ç‰¹å¾æ•£ç‚¹å›¾
        # æ‰¾åˆ°ä¸æˆ¿ä»·æœ€ç›¸å…³çš„ç‰¹å¾
        target_corr = corr_matrix['MedHouseVal'].abs().sort_values(ascending=False)
        most_correlated_feature = target_corr.index[1]  # é™¤äº†è‡ªå·±ä¹‹å¤–æœ€ç›¸å…³çš„
        
        axes[1,1].scatter(feature_data[most_correlated_feature], self.y, 
                         alpha=0.6, c='green', s=1)
        axes[1,1].set_xlabel(most_correlated_feature)
        axes[1,1].set_ylabel('MedHouseVal')
        axes[1,1].set_title(f'Most Correlated Feature: {most_correlated_feature}')
        axes[1,1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        analysis_plot_path = os.path.join(self.config.OUTPUT_DIR, 'extended_data_analysis.png')
        plt.savefig(analysis_plot_path, dpi=self.config.FIGURE_DPI, bbox_inches='tight')
        plt.close()
        
        if self.config.VERBOSE:
            print(f"ğŸ“Š æ•°æ®åˆ†æå›¾è¡¨å·²ä¿å­˜: {analysis_plot_path}")
    
    def run_core_performance_test(self):
        """è¿è¡Œæ ¸å¿ƒæ€§èƒ½æµ‹è¯•ï¼ˆå›ºå®š25%å™ªå£°ï¼‰"""
        if self.config.VERBOSE:
            print(f"ğŸ¯ è¿è¡Œæ ¸å¿ƒæ€§èƒ½æµ‹è¯• (å¼‚å¸¸æ¯”ä¾‹: {self.config.ANOMALY_RATIO:.1%})")
            print("=" * 60)
        
        # è¿è¡Œæ€§èƒ½å¯¹æ¯”
        results = self.benchmark.compare_models(
            X=self.X,
            y=self.y,
            task_type='regression',
            test_size=self.config.TEST_SIZE,
            val_size=self.config.VAL_SIZE,
            random_state=self.config.RANDOM_STATE,
            anomaly_ratio=self.config.ANOMALY_RATIO,
            
            # åŸºå‡†æ–¹æ³•é…ç½®
            baseline_methods=self.config.BASELINE_METHODS,
            
            # CausalEngineé…ç½®
            causal_modes=self.config.CAUSAL_MODES,
            
            # --- ç»Ÿä¸€å‚æ•°ï¼Œé€‚é…æ‰€æœ‰æ¨¡å‹ ---
            # ç¥ç»ç½‘ç»œç»“æ„ (é€‚é… 'hidden_sizes' å’Œ 'hidden_layer_sizes')
            hidden_sizes=self.config.NN_HIDDEN_SIZES,
            hidden_layer_sizes=self.config.NN_HIDDEN_SIZES,

            # è®­ç»ƒè½®æ•° (é€‚é… 'max_epochs' å’Œ 'max_iter')
            max_epochs=self.config.NN_MAX_EPOCHS,
            max_iter=self.config.NN_MAX_EPOCHS,

            # å­¦ä¹ ç‡ (é€‚é… 'lr' å’Œ 'learning_rate')
            lr=self.config.NN_LEARNING_RATE,
            learning_rate=self.config.NN_LEARNING_RATE,

            # æ—©åœå‚æ•°
            patience=self.config.NN_PATIENCE,
            tol=self.config.NN_TOLERANCE,
            
            # CausalEngineä¸“å±å‚æ•°
            gamma_init=self.config.CAUSAL_GAMMA_INIT,
            b_noise_init=self.config.CAUSAL_B_NOISE_INIT,
            b_noise_trainable=self.config.CAUSAL_B_NOISE_TRAINABLE,
            
            verbose=self.config.VERBOSE
        )
        
        self.results['core_performance'] = results
        
        if self.config.SAVE_PLOTS:
            self._create_performance_plots(results, 'core_performance')
        
        return results
    
    def run_robustness_test(self):
        """è¿è¡Œé²æ£’æ€§æµ‹è¯•ï¼ˆå¤šä¸ªå™ªå£°æ°´å¹³ï¼‰"""
        if not self.config.RUN_ROBUSTNESS_TEST:
            if self.config.VERBOSE:
                print("â­ï¸  è·³è¿‡é²æ£’æ€§æµ‹è¯• (RUN_ROBUSTNESS_TEST=False)")
            return None
        
        if self.config.VERBOSE:
            print(f"ğŸ›¡ï¸  è¿è¡Œé²æ£’æ€§æµ‹è¯•")
            print(f"   å™ªå£°æ°´å¹³: {[f'{r:.1%}' for r in self.config.ROBUSTNESS_ANOMALY_RATIOS]}")
            print("=" * 60)
        
        robustness_results = {}
        
        for i, anomaly_ratio in enumerate(self.config.ROBUSTNESS_ANOMALY_RATIOS):
            if self.config.VERBOSE:
                print(f"\\nğŸ”„ æµ‹è¯•å™ªå£°æ°´å¹³ {i+1}/{len(self.config.ROBUSTNESS_ANOMALY_RATIOS)}: {anomaly_ratio:.1%}")
            
            results = self.benchmark.compare_models(
                X=self.X,
                y=self.y,
                task_type='regression',
                test_size=self.config.TEST_SIZE,
                val_size=self.config.VAL_SIZE,
                random_state=self.config.RANDOM_STATE,
                anomaly_ratio=anomaly_ratio,
                
                # åŸºå‡†æ–¹æ³•é…ç½®
                baseline_methods=self.config.BASELINE_METHODS,
                
                # --- ç»Ÿä¸€å‚æ•°ï¼Œé€‚é…æ‰€æœ‰æ¨¡å‹ ---
                # CausalEngineé…ç½®
                causal_modes=self.config.CAUSAL_MODES,

                # ç¥ç»ç½‘ç»œç»“æ„ (é€‚é… 'hidden_sizes' å’Œ 'hidden_layer_sizes')
                hidden_sizes=self.config.NN_HIDDEN_SIZES,
                hidden_layer_sizes=self.config.NN_HIDDEN_SIZES,

                # è®­ç»ƒè½®æ•° (é€‚é… 'max_epochs' å’Œ 'max_iter')
                max_epochs=self.config.NN_MAX_EPOCHS,
                max_iter=self.config.NN_MAX_EPOCHS,

                # å­¦ä¹ ç‡ (é€‚é… 'lr' å’Œ 'learning_rate')
                lr=self.config.NN_LEARNING_RATE,
                learning_rate=self.config.NN_LEARNING_RATE,

                # æ—©åœå‚æ•°
                patience=self.config.NN_PATIENCE,
                tol=self.config.NN_TOLERANCE,

                # CausalEngineä¸“å±å‚æ•°
                gamma_init=self.config.CAUSAL_GAMMA_INIT,
                b_noise_init=self.config.CAUSAL_B_NOISE_INIT,
                b_noise_trainable=self.config.CAUSAL_B_NOISE_TRAINABLE,
                
                verbose=False  # é™ä½è¾“å‡ºé‡
            )
            
            robustness_results[anomaly_ratio] = results
        
        self.results['robustness'] = robustness_results
        
        if self.config.SAVE_PLOTS:
            self._create_robustness_plots(robustness_results)
        
        return robustness_results
    
    def _create_performance_plots(self, results, experiment_name):
        """åˆ›å»ºæ€§èƒ½å¯¹æ¯”å›¾è¡¨"""
        if self.config.VERBOSE:
            print(f"ğŸ“Š ç”Ÿæˆ{experiment_name}æ€§èƒ½å›¾è¡¨...")
        
        # åˆ›å»ºæ€§èƒ½å¯¹æ¯”å›¾
        fig, axes = plt.subplots(2, 2, figsize=self.config.FIGURE_SIZE_PERFORMANCE, dpi=self.config.FIGURE_DPI)
        fig.suptitle(f'Extended California Housing Test Set Performance\\nNoise Level: {self.config.ANOMALY_RATIO:.1%}', 
                    fontsize=16, fontweight='bold')
        
        # å‡†å¤‡æ•°æ®
        methods = list(results.keys())
        metrics = ['MAE', 'MdAE', 'RMSE', 'RÂ²']
        
        test_data = {metric: [results[method]['test'][metric] for method in methods] for metric in metrics}
        val_data = {metric: [results[method]['val'][metric] for method in methods] for metric in metrics}
        
        # è®¾ç½®é¢œè‰²
        colors = []
        for method in methods:
            if 'causal' in method.lower() or any(mode in method for mode in ['deterministic', 'standard', 'exogenous', 'endogenous']):
                colors.append('gold')  # CausalEngineç”¨é‡‘è‰²
            elif any(robust in method.lower() for robust in ['huber', 'cauchy', 'pinball']):
                colors.append('lightgreen')  # ç¨³å¥æ–¹æ³•ç”¨æµ…ç»¿
            else:
                colors.append('lightblue')  # ä¼ ç»Ÿæ–¹æ³•ç”¨æµ…è“
        
        # ä¸ºæ¯ä¸ªæŒ‡æ ‡åˆ›å»ºå­å›¾
        for i, metric in enumerate(metrics):
            ax = axes[i//2, i%2]
            
            x_pos = np.arange(len(methods))
            
            # åªæ˜¾ç¤ºæµ‹è¯•é›†æ€§èƒ½ï¼ˆæ›´æ¸…çˆ½ï¼‰
            bars = ax.bar(x_pos, test_data[metric], color=colors, alpha=0.8, edgecolor='black')
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for j, bar in enumerate(bars):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height,
                       f'{height:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')
            
            ax.set_title(f'{metric} (Test Set)', fontweight='bold')
            ax.set_ylabel(metric)
            ax.set_xticks(x_pos)
            ax.set_xticklabels(methods, rotation=45, ha='right')
            ax.grid(True, alpha=0.3)
            
            # é«˜äº®æœ€ä½³ç»“æœ
            if metric == 'RÂ²':
                best_idx = test_data[metric].index(max(test_data[metric]))
            else:
                best_idx = test_data[metric].index(min(test_data[metric]))
            bars[best_idx].set_color('gold')
            bars[best_idx].set_edgecolor('red')
            bars[best_idx].set_linewidth(2)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        performance_plot_path = os.path.join(self.config.OUTPUT_DIR, f'{experiment_name}_comparison.png')
        plt.savefig(performance_plot_path, dpi=self.config.FIGURE_DPI, bbox_inches='tight')
        plt.close()
        
        if self.config.VERBOSE:
            print(f"ğŸ“Š æ€§èƒ½å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜: {performance_plot_path}")
    
    def _create_robustness_plots(self, robustness_results):
        """åˆ›å»ºé²æ£’æ€§æµ‹è¯•å›¾è¡¨ - 4ä¸ªæŒ‡æ ‡çš„2x2å­å›¾å¸ƒå±€"""
        if self.config.VERBOSE:
            print("ğŸ“Š ç”Ÿæˆé²æ£’æ€§æµ‹è¯•å›¾è¡¨...")
        
        # åˆ›å»ºé²æ£’æ€§æµ‹è¯•å›¾
        fig, axes = plt.subplots(2, 2, figsize=self.config.FIGURE_SIZE_ROBUSTNESS, dpi=self.config.FIGURE_DPI)
        fig.suptitle('Extended Robustness Analysis: Performance vs Noise Level', fontsize=16, fontweight='bold')
        
        # å‡†å¤‡æ•°æ®
        noise_levels = list(robustness_results.keys())
        methods = list(robustness_results[noise_levels[0]].keys())
        metrics = ['MAE', 'MdAE', 'RMSE', 'RÂ²']
        metric_labels = ['Mean Absolute Error (MAE)', 'Median Absolute Error (MdAE)', 'Root Mean Squared Error (RMSE)', 'R-squared Score (RÂ²)']
        
        # è®¾ç½®é¢œè‰²å’Œçº¿å‹
        method_styles = {}
        causal_methods = [m for m in methods if 'causal' in m.lower() or any(mode in m for mode in ['deterministic', 'standard', 'exogenous', 'endogenous'])]
        robust_methods = [m for m in methods if any(robust in m.lower() for robust in ['huber', 'cauchy', 'pinball'])]
        traditional_methods = [m for m in methods if m not in causal_methods and m not in robust_methods]
        
        colors = ['#1f77b4', '#ff7f0e', '#d62728', '#2ca02c', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']
        markers = ['o', 's', 'v', '^', 'D', 'P', 'X', 'h', '+', '*']
        
        for i, method in enumerate(methods):
            if method in causal_methods:
                method_styles[method] = {'color': '#d62728', 'linestyle': '-', 'linewidth': 3, 'marker': 'o', 'markersize': 8}
            elif method in robust_methods:
                method_styles[method] = {'color': colors[i % len(colors)], 'linestyle': '--', 'linewidth': 2, 'marker': 's', 'markersize': 6}
            else:
                method_styles[method] = {'color': colors[i % len(colors)], 'linestyle': ':', 'linewidth': 2, 'marker': markers[i % len(markers)], 'markersize': 6}
        
        # ä¸ºæ¯ä¸ªæŒ‡æ ‡åˆ›å»ºå­å›¾
        for idx, (metric, metric_label) in enumerate(zip(metrics, metric_labels)):
            row, col = idx // 2, idx % 2
            ax = axes[row, col]
            
            for method in methods:
                # æ”¶é›†è¯¥æ–¹æ³•åœ¨å„å™ªå£°æ°´å¹³ä¸‹çš„æµ‹è¯•é›†æ€§èƒ½
                values = []
                for noise in noise_levels:
                    if method in robustness_results[noise]:
                        values.append(robustness_results[noise][method]['test'][metric])
                    else:
                        values.append(np.nan)
                
                ax.plot(noise_levels, values, 
                       label=method, 
                       **method_styles[method])
            
            ax.set_title(metric_label, fontsize=12, fontweight='bold')
            ax.set_xlabel('Label Noise Ratio', fontsize=11)
            ax.set_ylabel(metric, fontsize=11)
            ax.legend(fontsize=9, bbox_to_anchor=(1.05, 1), loc='upper left')
            ax.grid(True, alpha=0.3)
            
            # è®¾ç½®xè½´åˆ»åº¦æ ‡ç­¾
            ax.set_xticks(noise_levels)
            ax.set_xticklabels([f'{r:.1%}' for r in noise_levels])
            
            # ä¸ºRÂ²æ·»åŠ ç‰¹æ®Šå¤„ç†ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰ï¼Œå…¶ä»–æŒ‡æ ‡è¶Šä½è¶Šå¥½
            if metric == 'RÂ²':
                ax.set_ylim(bottom=0)  # RÂ²ä»0å¼€å§‹æ˜¾ç¤º
            else:
                ax.set_ylim(bottom=0)  # è¯¯å·®æŒ‡æ ‡ä»0å¼€å§‹æ˜¾ç¤º
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        robustness_plot_path = os.path.join(self.config.OUTPUT_DIR, 'extended_robustness_analysis.png')
        plt.savefig(robustness_plot_path, dpi=self.config.FIGURE_DPI, bbox_inches='tight')
        plt.close()
        
        if self.config.VERBOSE:
            print(f"ğŸ“Š é²æ£’æ€§åˆ†æå›¾è¡¨å·²ä¿å­˜: {robustness_plot_path}")
    
    def generate_summary_report(self):
        """ç”Ÿæˆå®éªŒæ€»ç»“æŠ¥å‘Š"""
        if self.config.VERBOSE:
            print("\\nğŸ“‹ ç”Ÿæˆå®éªŒæ€»ç»“æŠ¥å‘Š...")
        
        report_lines = []
        report_lines.append("# æ‰©å±•ç‰ˆåŠ å·æˆ¿ä»·å›å½’å®éªŒæ€»ç»“æŠ¥å‘Š")
        report_lines.append("")
        report_lines.append("ğŸ  **California Housing Dataset Regression Analysis**")
        report_lines.append("")
        report_lines.append("---")
        report_lines.append("")
        
        # å®éªŒé…ç½®
        report_lines.append("## ğŸ“Š å®éªŒé…ç½®")
        report_lines.append("")
        report_lines.append(f"- **æ•°æ®é›†**: åŠ å·æˆ¿ä»·æ•°æ®é›†")
        report_lines.append(f"  - æ ·æœ¬æ•°: {self.X.shape[0]:,}")
        report_lines.append(f"  - ç‰¹å¾æ•°: {self.X.shape[1]}")
        report_lines.append(f"  - æˆ¿ä»·èŒƒå›´: ${self.y.min():.2f} - ${self.y.max():.2f} (10ä¸‡ç¾å…ƒ)")
        report_lines.append("")
        report_lines.append(f"- **æ•°æ®åˆ†å‰²**:")
        report_lines.append(f"  - æµ‹è¯•é›†æ¯”ä¾‹: {self.config.TEST_SIZE:.1%}")
        report_lines.append(f"  - éªŒè¯é›†æ¯”ä¾‹: {self.config.VAL_SIZE:.1%}")
        report_lines.append(f"  - éšæœºç§å­: {self.config.RANDOM_STATE}")
        report_lines.append("")
        report_lines.append(f"- **ç¥ç»ç½‘ç»œç»Ÿä¸€é…ç½®**:")
        report_lines.append(f"  - ç½‘ç»œç»“æ„: {self.config.NN_HIDDEN_SIZES}")
        report_lines.append(f"  - æœ€å¤§è½®æ•°: {self.config.NN_MAX_EPOCHS}")
        report_lines.append(f"  - å­¦ä¹ ç‡: {self.config.NN_LEARNING_RATE}")
        report_lines.append(f"  - æ—©åœpatience: {self.config.NN_PATIENCE}")
        report_lines.append("")
        report_lines.append(f"- **å®éªŒæ–¹æ³•**: {len(self.config.BASELINE_METHODS) + len(self.config.CAUSAL_MODES)} ç§")
        report_lines.append(f"  - ä¼ ç»Ÿæ–¹æ³• ({len(self.config.BASELINE_METHODS)}ç§): {', '.join(self.config.BASELINE_METHODS)}")
        report_lines.append(f"  - CausalEngine ({len(self.config.CAUSAL_MODES)}ç§): {', '.join(self.config.CAUSAL_MODES)}")
        report_lines.append("")
        
        # æ ¸å¿ƒæ€§èƒ½æµ‹è¯•ç»“æœ
        if 'core_performance' in self.results:
            results = self.results['core_performance']
            report_lines.append("## ğŸ¯ æ ¸å¿ƒæ€§èƒ½æµ‹è¯•ç»“æœ")
            report_lines.append("")
            report_lines.append(f"**å™ªå£°æ°´å¹³**: {self.config.ANOMALY_RATIO:.1%}")
            report_lines.append("")
            
            # åˆ›å»ºæ€§èƒ½è¡¨æ ¼ - æŒ‰MdAEæ’åº
            methods_by_mdae = sorted(results.keys(), key=lambda x: results[x]['test']['MdAE'])
            
            report_lines.append("### ğŸ“ˆ æµ‹è¯•é›†æ€§èƒ½æ’å (æŒ‰MdAEå‡åº)")
            report_lines.append("")
            
            # è¡¨æ ¼å¤´
            report_lines.append("| æ’å | æ–¹æ³• | MAE | MdAE | RMSE | RÂ² | æ–¹æ³•ç±»å‹ |")
            report_lines.append("|:----:|------|----:|-----:|-----:|---:|----------|")
            
            for i, method in enumerate(methods_by_mdae, 1):
                test_metrics = results[method]['test']
                
                # åˆ¤æ–­æ–¹æ³•ç±»å‹
                if any(mode in method for mode in ['deterministic', 'standard', 'exogenous', 'endogenous']):
                    method_type = "ğŸ¤– CausalEngine"
                elif any(robust in method.lower() for robust in ['huber', 'cauchy', 'pinball']):
                    method_type = "ğŸ›¡ï¸ ç¨³å¥å›å½’"
                elif method.lower() in ['catboost', 'random_forest']:
                    method_type = "ğŸŒ² é›†æˆå­¦ä¹ "
                else:
                    method_type = "ğŸ§  ç¥ç»ç½‘ç»œ"
                
                report_lines.append(f"| {i} | **{method}** | "
                                  f"{test_metrics['MAE']:.4f} | "
                                  f"**{test_metrics['MdAE']:.4f}** | "
                                  f"{test_metrics['RMSE']:.4f} | "
                                  f"{test_metrics['RÂ²']:.4f} | "
                                  f"{method_type} |")
            
            report_lines.append("")
            
            # éªŒè¯é›†vsæµ‹è¯•é›†å¯¹æ¯”ï¼ˆå±•ç¤ºå™ªå£°å½±å“ï¼‰
            report_lines.append("### ğŸ” éªŒè¯é›† vs æµ‹è¯•é›†æ€§èƒ½å¯¹æ¯”")
            report_lines.append("")
            report_lines.append("*éªŒè¯é›†åŒ…å«å™ªå£°ï¼Œæµ‹è¯•é›†ä¸ºçº¯å‡€æ•°æ®*")
            report_lines.append("")
            
            report_lines.append("| æ–¹æ³• | éªŒè¯é›†MdAE | æµ‹è¯•é›†MdAE | æ€§èƒ½æå‡ |")
            report_lines.append("|------|----------:|----------:|--------:|")
            
            for method in methods_by_mdae:
                val_mdae = results[method]['val']['MdAE']
                test_mdae = results[method]['test']['MdAE']
                improvement = ((val_mdae - test_mdae) / val_mdae) * 100
                
                report_lines.append(f"| {method} | "
                                  f"{val_mdae:.4f} | "
                                  f"{test_mdae:.4f} | "
                                  f"{improvement:+.1f}% |")
            
            report_lines.append("")
            
            # å…³é”®å‘ç°
            best_mdae_method = methods_by_mdae[0]
            best_mdae_score = results[best_mdae_method]['test']['MdAE']
            
            # è¯†åˆ«CausalEngineæ–¹æ³•
            causal_methods = [m for m in results.keys() if any(mode in m for mode in ['deterministic', 'standard', 'exogenous', 'endogenous'])]
            
            report_lines.append("### ğŸ† å…³é”®å‘ç°")
            report_lines.append("")
            report_lines.append(f"- **ğŸ¥‡ æœ€ä½³æ•´ä½“æ€§èƒ½**: `{best_mdae_method}` (MdAE: {best_mdae_score:.4f})")
            
            if causal_methods:
                best_causal = min(causal_methods, key=lambda x: results[x]['test']['MdAE'])
                causal_rank = methods_by_mdae.index(best_causal) + 1
                causal_score = results[best_causal]['test']['MdAE']
                report_lines.append(f"- **ğŸ¤– æœ€ä½³CausalEngine**: `{best_causal}` (æ’å: {causal_rank}/{len(methods_by_mdae)}, MdAE: {causal_score:.4f})")
                
                # CausalEngineæ¨¡å¼å¯¹æ¯”
                if len(causal_methods) > 1:
                    report_lines.append("")
                    report_lines.append("**CausalEngineæ¨¡å¼å¯¹æ¯”**:")
                    for causal_method in sorted(causal_methods, key=lambda x: results[x]['test']['MdAE']):
                        rank = methods_by_mdae.index(causal_method) + 1
                        score = results[causal_method]['test']['MdAE']
                        report_lines.append(f"  - `{causal_method}`: æ’å {rank}, MdAE {score:.4f}")
            
            # ä¼ ç»Ÿæ–¹æ³•åˆ†æ
            traditional_methods = [m for m in results.keys() if m not in causal_methods]
            if traditional_methods:
                best_traditional = min(traditional_methods, key=lambda x: results[x]['test']['MdAE'])
                traditional_rank = methods_by_mdae.index(best_traditional) + 1
                traditional_score = results[best_traditional]['test']['MdAE']
                report_lines.append(f"- **ğŸ… æœ€ä½³ä¼ ç»Ÿæ–¹æ³•**: `{best_traditional}` (æ’å: {traditional_rank}/{len(methods_by_mdae)}, MdAE: {traditional_score:.4f})")
            
            report_lines.append("")
        
        # é²æ£’æ€§æµ‹è¯•ç»“æœ
        if 'robustness' in self.results:
            robustness_results = self.results['robustness']
            report_lines.append("## ğŸ›¡ï¸ é²æ£’æ€§æµ‹è¯•ç»“æœ")
            report_lines.append("")
            
            noise_levels = sorted(robustness_results.keys())
            methods = list(robustness_results[noise_levels[0]].keys())
            
            report_lines.append("### ğŸ“Š MdAEæ€§èƒ½éšå™ªå£°æ°´å¹³å˜åŒ–")
            report_lines.append("")
            
            # è¡¨æ ¼å¤´
            header = "| æ–¹æ³• | " + " | ".join([f"{r:.0%}" for r in noise_levels]) + " | ç¨³å®šæ€§* |"
            separator = "|" + "|".join(["-" * max(6, len(f"{r:.0%}")) for r in [0] + noise_levels + [0]]) + "|"
            separator = "|------|" + "|".join([f"{'-'*(len(f'{r:.0%}')+1):->6}" for r in noise_levels]) + "|--------|"
            
            report_lines.append(header)
            report_lines.append(separator)
            
            # æŒ‰0%å™ªå£°æ€§èƒ½æ’åº
            methods_by_clean = sorted(methods, key=lambda x: robustness_results[0.0][x]['test']['MdAE'])
            
            for method in methods_by_clean:
                mdae_values = []
                scores = []
                for noise in noise_levels:
                    score = robustness_results[noise][method]['test']['MdAE']
                    scores.append(score)
                    mdae_values.append(f"{score:.4f}")
                
                # è®¡ç®—ç¨³å®šæ€§ (æœ€å¤§å€¼-æœ€å°å€¼)/æœ€å°å€¼
                stability = (max(scores) - min(scores)) / min(scores) * 100
                
                # æ–¹æ³•åæ ¼å¼åŒ–
                method_display = f"**{method}**" if any(mode in method for mode in ['deterministic', 'standard']) else method
                
                report_lines.append(f"| {method_display} | " + 
                                  " | ".join(mdae_values) + 
                                  f" | {stability:.1f}% |")
            
            report_lines.append("")
            report_lines.append("*ç¨³å®šæ€§ = (æœ€å¤§MdAE - æœ€å°MdAE) / æœ€å°MdAE Ã— 100%ï¼Œè¶Šå°è¶Šç¨³å®š*")
            report_lines.append("")
            
            # é²æ£’æ€§åˆ†æ
            report_lines.append("### ğŸ” é²æ£’æ€§åˆ†æ")
            report_lines.append("")
            
            # æ‰¾å‡ºæœ€ç¨³å®šçš„æ–¹æ³•
            stability_scores = {}
            for method in methods:
                scores = [robustness_results[noise][method]['test']['MdAE'] for noise in noise_levels]
                stability_scores[method] = (max(scores) - min(scores)) / min(scores) * 100
            
            most_stable = min(stability_scores.keys(), key=lambda x: stability_scores[x])
            least_stable = max(stability_scores.keys(), key=lambda x: stability_scores[x])
            
            report_lines.append(f"- **ğŸ† æœ€ç¨³å®šæ–¹æ³•**: `{most_stable}` (ç¨³å®šæ€§: {stability_scores[most_stable]:.1f}%)")
            report_lines.append(f"- **âš ï¸ æœ€ä¸ç¨³å®šæ–¹æ³•**: `{least_stable}` (ç¨³å®šæ€§: {stability_scores[least_stable]:.1f}%)")
            
            report_lines.append("")
        
        # æ·»åŠ è„šæ³¨
        report_lines.append("---")
        report_lines.append("")
        report_lines.append("## ğŸ“ è¯´æ˜")
        report_lines.append("")
        report_lines.append("- **MdAE**: Median Absolute Error (ä¸­ä½æ•°ç»å¯¹è¯¯å·®) - ä¸»è¦è¯„ä¼°æŒ‡æ ‡")
        report_lines.append("- **MAE**: Mean Absolute Error (å¹³å‡ç»å¯¹è¯¯å·®)")
        report_lines.append("- **RMSE**: Root Mean Square Error (å‡æ–¹æ ¹è¯¯å·®)")
        report_lines.append("- **RÂ²**: å†³å®šç³»æ•° (è¶Šæ¥è¿‘1è¶Šå¥½)")
        report_lines.append("- **å™ªå£°è®¾ç½®**: éªŒè¯é›†åŒ…å«äººå·¥å™ªå£°ï¼Œæµ‹è¯•é›†ä¸ºçº¯å‡€æ•°æ®")
        report_lines.append("- **ç»Ÿä¸€é…ç½®**: æ‰€æœ‰ç¥ç»ç½‘ç»œæ–¹æ³•ä½¿ç”¨ç›¸åŒçš„è¶…å‚æ•°ç¡®ä¿å…¬å¹³æ¯”è¾ƒ")
        report_lines.append("")
        report_lines.append(f"ğŸ“Š **ç”Ÿæˆæ—¶é—´**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = os.path.join(self.config.OUTPUT_DIR, 'extended_experiment_summary.md')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write('\\n'.join(report_lines))
        
        if self.config.VERBOSE:
            print(f"ğŸ“‹ å®éªŒæ€»ç»“æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        return report_lines


def main():
    """ä¸»å‡½æ•° - è¿è¡Œæ‰©å±•ç‰ˆæ•™ç¨‹"""
    print("ğŸš€ æ‰©å±•ç‰ˆåŠ å·æˆ¿ä»·å›å½’æ•™ç¨‹")
    print("=" * 60)
    
    # åˆ›å»ºæ•™ç¨‹å®ä¾‹
    tutorial = ExtendedCaliforniaHousingTutorial()
    
    # 1. åŠ è½½å’Œåˆ†ææ•°æ®
    tutorial.load_and_analyze_data()
    
    # 2. è¿è¡Œæ ¸å¿ƒæ€§èƒ½æµ‹è¯•
    core_results = tutorial.run_core_performance_test()
    
    # 3. è¿è¡Œé²æ£’æ€§æµ‹è¯•
    robustness_results = tutorial.run_robustness_test()
    
    # 4. ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    tutorial.generate_summary_report()
    
    if tutorial.config.VERBOSE:
        print("\\nğŸ‰ æ‰©å±•ç‰ˆæ•™ç¨‹è¿è¡Œå®Œæˆï¼")
        print(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {tutorial.config.OUTPUT_DIR}")
        print("\\nä¸»è¦è¾“å‡ºæ–‡ä»¶:")
        print("- extended_data_analysis.png: æ•°æ®åˆ†æå›¾è¡¨")
        print("- core_performance_comparison.png: æ ¸å¿ƒæ€§èƒ½å¯¹æ¯”å›¾è¡¨")
        if tutorial.config.RUN_ROBUSTNESS_TEST:
            print("- extended_robustness_analysis.png: é²æ£’æ€§åˆ†æå›¾è¡¨")
        print("- extended_experiment_summary.md: å®éªŒæ€»ç»“æŠ¥å‘Š")


if __name__ == "__main__":
    main()