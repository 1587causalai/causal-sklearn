"""
åŸºå‡†æµ‹è¯•æ–¹æ³•é…ç½®æ¨¡å—

å®šä¹‰æ‰€æœ‰ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•çš„é»˜è®¤å‚æ•°é…ç½®ï¼Œæ”¯æŒé€šè¿‡é…ç½®çµæ´»é€‰æ‹©å’Œè°ƒæ•´åŸºå‡†æ–¹æ³•ã€‚
"""

# ğŸ§  ç»Ÿä¸€ç¥ç»ç½‘ç»œå‚æ•°é…ç½® - åœ¨æ­¤å¤„ä¿®æ”¹æ‰€æœ‰ç¥ç»ç½‘ç»œæ–¹æ³•çš„å…±åŒå‚æ•°
# =========================================================================
# ğŸ”§ ä¿®æ”¹è¿™äº›å˜é‡ä¼šå½±å“æ‰€æœ‰ç¥ç»ç½‘ç»œæ–¹æ³•çš„å‚æ•°ï¼
NN_HIDDEN_SIZES = (128, 64, 32)                 # ç¥ç»ç½‘ç»œéšè—å±‚ç»“æ„
NN_MAX_EPOCHS = 3000                            # æœ€å¤§è®­ç»ƒè½®æ•°
NN_LEARNING_RATE = 0.01                         # å­¦ä¹ ç‡
NN_PATIENCE = 200                               # æ—©åœpatience
NN_TOLERANCE = 1e-4                             # æ—©åœtolerance
# =========================================================================

# é»˜è®¤åŸºå‡†æ–¹æ³•é…ç½®
DEFAULT_METHOD_CONFIGS = {
    # === ç¥ç»ç½‘ç»œæ–¹æ³• ===
    'sklearn_mlp': {
        'name': 'sklearn MLP',
        'type': 'neural_network',
        'params': {
            'hidden_layer_sizes': NN_HIDDEN_SIZES,
            'max_iter': NN_MAX_EPOCHS,
            'learning_rate_init': NN_LEARNING_RATE,
            'early_stopping': True,
            'validation_fraction': 0.2,
            'n_iter_no_change': NN_PATIENCE,
            'alpha': 0.0,
            'random_state': 42,
            'tol': NN_TOLERANCE,
        }
    },
    
    'pytorch_mlp': {
        'name': 'PyTorch MLP',
        'type': 'neural_network',
        'params': {
            'hidden_layer_sizes': NN_HIDDEN_SIZES,
            'max_iter': NN_MAX_EPOCHS,
            'learning_rate': NN_LEARNING_RATE,
            'early_stopping': True,
            'validation_fraction': 0.2,
            'n_iter_no_change': NN_PATIENCE,
            'tol': NN_TOLERANCE,
            'alpha': 0.0,
            'random_state': 42,
            'batch_size': None
        }
    },
    
    'mlp_huber': {
        'name': 'MLP Huber',
        'type': 'robust_neural_network',
        'params': {
            'hidden_layer_sizes': NN_HIDDEN_SIZES,
            'max_iter': NN_MAX_EPOCHS,
            'learning_rate': NN_LEARNING_RATE,
            'early_stopping': True,
            'validation_fraction': 0.2,
            'n_iter_no_change': NN_PATIENCE,
            'tol': NN_TOLERANCE,
            'alpha': 0.0,
            'random_state': 42,
            'batch_size': None,
            'delta': 1.0  # HuberæŸå¤±çš„deltaå‚æ•°
        }
    },
    
    'mlp_pinball_median': {
        'name': 'MLP Pinball Median',
        'type': 'robust_neural_network', 
        'params': {
            'hidden_layer_sizes': NN_HIDDEN_SIZES,
            'max_iter': NN_MAX_EPOCHS,
            'learning_rate': NN_LEARNING_RATE,
            'early_stopping': True,
            'validation_fraction': 0.2,
            'n_iter_no_change': NN_PATIENCE,
            'tol': NN_TOLERANCE,
            'alpha': 0.0,
            'random_state': 42,
            'batch_size': None,
            'quantile': 0.5  # ä¸­ä½æ•°å›å½’ (50%åˆ†ä½æ•°)
        }
    },
    
    'mlp_cauchy': {
        'name': 'MLP Cauchy',
        'type': 'robust_neural_network',
        'params': {
            'hidden_layer_sizes': NN_HIDDEN_SIZES,
            'max_iter': NN_MAX_EPOCHS,
            'learning_rate': NN_LEARNING_RATE,
            'early_stopping': True,
            'validation_fraction': 0.2,
            'n_iter_no_change': NN_PATIENCE,
            'tol': NN_TOLERANCE,
            'alpha': 0.0,
            'random_state': 42,
            'batch_size': None
        }
    },
    
    # === é›†æˆæ–¹æ³• ===
    'random_forest': {
        'name': 'Random Forest',
        'type': 'ensemble',
        'params': {
            'n_estimators': 100,
            'max_depth': 15,
            'min_samples_split': 5,
            'min_samples_leaf': 2,
            'max_features': 'sqrt',
            'random_state': 42,
            'n_jobs': -1
        }
    },
    
    'extra_trees': {
        'name': 'Extra Trees',
        'type': 'ensemble',
        'params': {
            'n_estimators': 100,
            'max_depth': 15,
            'min_samples_split': 5,
            'min_samples_leaf': 2,
            'max_features': 'sqrt',
            'random_state': 42,
            'n_jobs': -1
        }
    },
    
    'gradient_boosting': {
        'name': 'Gradient Boosting',
        'type': 'ensemble',
        'params': {
            'n_estimators': 100,
            'learning_rate': 0.1,
            'max_depth': 6,
            'min_samples_split': 5,
            'min_samples_leaf': 2,
            'subsample': 0.8,
            'random_state': 42
        }
    },
    
    'xgboost': {
        'name': 'XGBoost',
        'type': 'boosting',
        'params': {
            'n_estimators': 100,
            'learning_rate': 0.1,
            'max_depth': 6,
            'min_child_weight': 1,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'random_state': 42,
            'n_jobs': -1,
            'verbosity': 0
        }
    },
    
    'lightgbm': {
        'name': 'LightGBM',
        'type': 'boosting',
        'params': {
            'n_estimators': 100,
            'learning_rate': 0.1,
            'max_depth': 6,
            'num_leaves': 31,
            'min_child_samples': 20,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'random_state': 42,
            'n_jobs': -1,
            'verbosity': -1
        }
    },
    
    'catboost': {
        'name': 'CatBoost',
        'type': 'boosting',
        'params': {
            'iterations': 100,
            'learning_rate': 0.1,
            'depth': 6,
            'l2_leaf_reg': 3,
            'random_state': 42,
            'verbose': False,
            'thread_count': -1
        }
    },
    
    # === æ”¯æŒå‘é‡æœº ===
    'svm_rbf': {
        'name': 'SVM (RBF)',
        'type': 'svm',
        'params': {
            'kernel': 'rbf',
            'C': 1.0,
            'gamma': 'scale',
            'tol': 1e-3,
            'max_iter': 1000
        }
    },
    
    'svm_linear': {
        'name': 'SVM (Linear)',
        'type': 'svm',
        'params': {
            'kernel': 'linear',
            'C': 1.0,
            'tol': 1e-3,
            'max_iter': 1000
        }
    },
    
    'svm_poly': {
        'name': 'SVM (Poly)',
        'type': 'svm',
        'params': {
            'kernel': 'poly',
            'degree': 3,
            'C': 1.0,
            'gamma': 'scale',
            'tol': 1e-3,
            'max_iter': 1000
        }
    },
    
    # === çº¿æ€§æ–¹æ³• ===
    'linear_regression': {
        'name': 'Linear Regression',
        'type': 'linear',
        'params': {}
    },
    
    'ridge': {
        'name': 'Ridge Regression',
        'type': 'linear',
        'params': {
            'alpha': 1.0,
            'max_iter': 1000,
            'tol': 1e-3,
            'random_state': 42
        }
    },
    
    'lasso': {
        'name': 'Lasso Regression',
        'type': 'linear',
        'params': {
            'alpha': 1.0,
            'max_iter': 1000,
            'tol': 1e-3,
            'random_state': 42
        }
    },
    
    'elastic_net': {
        'name': 'Elastic Net',
        'type': 'linear',
        'params': {
            'alpha': 1.0,
            'l1_ratio': 0.5,
            'max_iter': 1000,
            'tol': 1e-3,
            'random_state': 42
        }
    },
    
    # === Kè¿‘é‚» ===
    'knn': {
        'name': 'K-Nearest Neighbors',
        'type': 'neighbor',
        'params': {
            'n_neighbors': 5,
            'weights': 'uniform',
            'algorithm': 'auto',
            'n_jobs': -1
        }
    },
    
    # === å†³ç­–æ ‘ ===
    'decision_tree': {
        'name': 'Decision Tree',
        'type': 'tree',
        'params': {
            'max_depth': 10,
            'min_samples_split': 5,
            'min_samples_leaf': 2,
            'random_state': 42
        }
    }
}

# é¢„å®šä¹‰æ–¹æ³•ç»„åˆ
METHOD_GROUPS = {
    # åŸºç¡€ç»„åˆï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
    'basic': ['sklearn_mlp', 'pytorch_mlp', 'random_forest', 'xgboost'],
    
    # ç¥ç»ç½‘ç»œç»„åˆ
    'neural': ['sklearn_mlp', 'pytorch_mlp', 'mlp_huber', 'mlp_pinball_median', 'mlp_cauchy'],
    
    # é›†æˆæ–¹æ³•ç»„åˆ
    'ensemble': ['random_forest', 'extra_trees', 'gradient_boosting'],
    
    # æå‡æ–¹æ³•ç»„åˆ
    'boosting': ['xgboost', 'lightgbm', 'catboost'],
    
    # æ”¯æŒå‘é‡æœºç»„åˆ
    'svm': ['svm_rbf', 'svm_linear', 'svm_poly'],
    
    # çº¿æ€§æ–¹æ³•ç»„åˆ
    'linear': ['linear_regression', 'ridge', 'lasso', 'elastic_net'],
    
    # å…¨é¢æµ‹è¯•ç»„åˆ
    'comprehensive': [
        'sklearn_mlp', 'pytorch_mlp', 'random_forest', 'extra_trees', 
        'gradient_boosting', 'xgboost', 'lightgbm', 'svm_rbf', 'knn'
    ],
    
    # ç«äº‰æ€§åŸºå‡†ç»„åˆï¼ˆæœ€å¼ºæ–¹æ³•ï¼‰
    'competitive': ['pytorch_mlp', 'random_forest', 'xgboost', 'lightgbm', 'svm_rbf'],
    
    # è½»é‡çº§æµ‹è¯•ç»„åˆï¼ˆå¿«é€ŸéªŒè¯ï¼‰
    'lightweight': ['sklearn_mlp', 'random_forest', 'xgboost']
}

# ä»»åŠ¡ç±»å‹ç‰¹å®šçš„æ¨èæ–¹æ³•
TASK_SPECIFIC_RECOMMENDATIONS = {
    'regression': {
        'default': ['sklearn_mlp', 'pytorch_mlp', 'random_forest', 'xgboost', 'svm_rbf'],
        'robust': ['mlp_huber', 'mlp_pinball_median', 'mlp_cauchy', 'random_forest', 'extra_trees', 'gradient_boosting'],  # å¯¹å¼‚å¸¸å€¼é²æ£’
        'fast': ['sklearn_mlp', 'random_forest', 'ridge'],  # å¿«é€Ÿè®­ç»ƒ
        'accurate': ['pytorch_mlp', 'xgboost', 'lightgbm', 'svm_rbf']  # é«˜ç²¾åº¦
    },
    'classification': {
        'default': ['sklearn_mlp', 'pytorch_mlp', 'random_forest', 'xgboost', 'svm_rbf'],
        'robust': ['random_forest', 'extra_trees', 'gradient_boosting', 'svm_rbf'],
        'fast': ['sklearn_mlp', 'random_forest', 'knn'],
        'accurate': ['pytorch_mlp', 'xgboost', 'lightgbm', 'svm_rbf']
    }
}

def get_method_config(method_name):
    """è·å–æŒ‡å®šæ–¹æ³•çš„é…ç½®"""
    return DEFAULT_METHOD_CONFIGS.get(method_name, None)

def get_method_group(group_name):
    """è·å–é¢„å®šä¹‰çš„æ–¹æ³•ç»„åˆ"""
    return METHOD_GROUPS.get(group_name, [])

def get_task_recommendations(task_type, recommendation_type='default'):
    """è·å–ä»»åŠ¡ç‰¹å®šçš„æ¨èæ–¹æ³•"""
    task_recs = TASK_SPECIFIC_RECOMMENDATIONS.get(task_type, {})
    return task_recs.get(recommendation_type, task_recs.get('default', []))

def list_available_methods():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„åŸºå‡†æ–¹æ³•"""
    return list(DEFAULT_METHOD_CONFIGS.keys())

def list_available_groups():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ–¹æ³•ç»„åˆ"""
    return list(METHOD_GROUPS.keys())

def validate_methods(method_list):
    """éªŒè¯æ–¹æ³•åˆ—è¡¨æ˜¯å¦éƒ½æ˜¯å¯ç”¨çš„"""
    available = set(DEFAULT_METHOD_CONFIGS.keys())
    invalid = [m for m in method_list if m not in available]
    
    if invalid:
        raise ValueError(f"æœªçŸ¥çš„åŸºå‡†æ–¹æ³•: {invalid}. å¯ç”¨æ–¹æ³•: {list(available)}")
    
    return True

def expand_method_groups(method_list):
    """å±•å¼€æ–¹æ³•ç»„åˆä¸ºå…·ä½“æ–¹æ³•åˆ—è¡¨"""
    expanded = []
    
    for item in method_list:
        if item.startswith('group:'):
            # å¤„ç†ç»„åˆå¼•ç”¨ï¼Œå¦‚ 'group:basic'
            group_name = item[6:]  # å»æ‰ 'group:' å‰ç¼€
            if group_name in METHOD_GROUPS:
                expanded.extend(METHOD_GROUPS[group_name])
            else:
                raise ValueError(f"æœªçŸ¥çš„æ–¹æ³•ç»„åˆ: {group_name}")
        else:
            # æ™®é€šæ–¹æ³•å
            expanded.append(item)
    
    # å»é‡å¹¶ä¿æŒé¡ºåº
    seen = set()
    result = []
    for method in expanded:
        if method not in seen:
            seen.add(method)
            result.append(method)
    
    return result