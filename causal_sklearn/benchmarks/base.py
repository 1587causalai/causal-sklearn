"""
CausalEngineåŸºå‡†æµ‹è¯•åŸºç¡€æ¨¡å— - å…¨å±€æ ‡å‡†åŒ–é‡æ„ç‰ˆ
===================================================

æä¾›ç»Ÿä¸€çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œç”¨äºæ¯”è¾ƒCausalEngineä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•çš„æ€§èƒ½ã€‚
æ­¤ç‰ˆæœ¬ç»è¿‡é‡æ„ï¼Œéµå¾ª"å…¨å±€æ ‡å‡†åŒ–"å’Œ"èŒè´£åˆ†ç¦»"åŸåˆ™ï¼Œç¡®ä¿å®éªŒçš„å…¬å¹³æ€§å’Œå¯å¤ç°æ€§ã€‚
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.neural_network import MLPRegressor, MLPClassifier
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, median_absolute_error
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import os
import warnings

from .._causal_engine import create_causal_regressor, create_causal_classifier
from .methods import BaselineMethodFactory, MethodDependencyChecker, filter_available_methods
from ..data_processing import inject_shuffle_noise
from .method_configs import (
    DEFAULT_METHOD_CONFIGS, get_method_group, get_task_recommendations, 
    validate_methods, expand_method_groups, list_available_methods
)

warnings.filterwarnings('ignore')


class PyTorchBaseline(nn.Module):
    """PyTorchåŸºçº¿æ¨¡å‹ï¼ˆä¼ ç»ŸMLPï¼‰"""
    
    def __init__(self, input_size, output_size, hidden_sizes=(128, 64)):
        super().__init__()
        
        layers = []
        prev_size = input_size
        
        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.ReLU(),
            ])
            prev_size = hidden_size
        
        layers.append(nn.Linear(prev_size, output_size))
        
        self.network = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.network(x)


class BaselineBenchmark:
    """
    åŸºå‡†æµ‹è¯•åŸºç±»
    
    æä¾›ç»Ÿä¸€çš„æ¥å£æ¥æ¯”è¾ƒCausalEngineä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•çš„æ€§èƒ½ã€‚
    æ”¯æŒé…ç½®é©±åŠ¨çš„åŸºå‡†æ–¹æ³•é€‰æ‹©ï¼ŒåŒ…æ‹¬ç¥ç»ç½‘ç»œã€é›†æˆæ–¹æ³•ã€SVMã€çº¿æ€§æ–¹æ³•ç­‰ã€‚
    """
    
    def __init__(self):
        self.results = {}
        self.method_factory = BaselineMethodFactory()
        self.dependency_checker = MethodDependencyChecker()
    
    def train_causal_engine(self, X_train, y_train, X_val, y_val, task_type='regression', mode='standard',
                           hidden_sizes=(128, 64), max_epochs=5000, lr=0.01, patience=500, tol=1e-8,
                           gamma_init=1.0, b_noise_init=1.0, b_noise_trainable=True, ovr_threshold=0.0, verbose=True):
        """è®­ç»ƒCausalEngineæ¨¡å‹"""
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        input_size = X_train.shape[1]
        if task_type == 'regression':
            output_size = 1
            model = create_causal_regressor(
                input_size=input_size,
                output_size=output_size,
                repre_size=hidden_sizes[0] if hidden_sizes else None,
                causal_size=hidden_sizes[0] if hidden_sizes else None,
                perception_hidden_layers=hidden_sizes,
                abduction_hidden_layers=(),
                gamma_init=gamma_init,
                b_noise_init=b_noise_init,
                b_noise_trainable=b_noise_trainable
            )
        else:
            # ç¡®å®šç±»åˆ«æ•°é‡æ—¶ï¼Œä½¿ç”¨ä¼ é€’è¿›æ¥çš„y_trainï¼Œå®ƒå¯èƒ½å·²ç»æ˜¯LabelEncoderä¹‹åçš„ç»“æœ
            n_classes = len(np.unique(y_train))
            model = create_causal_classifier(
                input_size=input_size,
                n_classes=n_classes,
                repre_size=hidden_sizes[0] if hidden_sizes else None,
                causal_size=hidden_sizes[0] if hidden_sizes else None,
                perception_hidden_layers=hidden_sizes,
                abduction_hidden_layers=(),
                gamma_init=gamma_init,
                b_noise_init=b_noise_init,
                b_noise_trainable=b_noise_trainable,
                ovr_threshold=ovr_threshold
            )
        
        if verbose:
            print(f"   ä¸ºæ¨¡å¼æ„å»ºæ¨¡å‹: {mode}")
            print(f"   ==> æ¨¡å‹å·²æ„å»ºã€‚æ€»å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in model.parameters() if p.requires_grad)}")
        
        model = model.to(device)
        
        X_train_torch = torch.FloatTensor(X_train).to(device)
        y_train_torch = torch.FloatTensor(y_train).to(device)
        X_val_torch = torch.FloatTensor(X_val).to(device)
        y_val_torch = torch.FloatTensor(y_val).to(device)
        
        if task_type == 'classification':
            y_train_torch = y_train_torch.long()
            y_val_torch = y_val_torch.long()
        else:
            if len(y_train_torch.shape) == 1:
                y_train_torch = y_train_torch.unsqueeze(1)
                y_val_torch = y_val_torch.unsqueeze(1)
        
        optimizer = optim.Adam(model.parameters(), lr=lr)
        
        best_val_loss = float('inf')
        patience_counter = 0
        best_state_dict = None
        
        for epoch in range(max_epochs):
            model.train()
            optimizer.zero_grad()
            loss = model.compute_loss(X_train_torch, y_train_torch, mode)
            loss.backward()
            optimizer.step()
            
            model.eval()
            with torch.no_grad():
                val_loss = model.compute_loss(X_val_torch, y_val_torch, mode).item()
            
            if epoch % 500 == 0 and verbose:
                print(f"      Epoch {epoch}: Train Loss = {loss.item():.6f}, Val Loss = {val_loss:.6f}")
            
            if val_loss < best_val_loss - tol:
                best_val_loss = val_loss
                patience_counter = 0
                best_state_dict = model.state_dict().copy()
            else:
                patience_counter += 1
            
            if patience_counter >= patience:
                if verbose:
                    print(f"   Early stopping at epoch {epoch + 1}. Best validation loss: {best_val_loss:.6f}")
                if best_state_dict:
                    model.load_state_dict(best_state_dict)
                break
        
        return model

    def compare_models(self, X, y, task_type='regression', test_size=0.2, val_size=0.25,
                       anomaly_ratio=0.0, random_state=42, verbose=True, **kwargs):
        """
        é€šç”¨æ¨¡å‹æ¯”è¾ƒæ–¹æ³• - å…¨å±€æ ‡å‡†åŒ–é‡æ„ç‰ˆ

        æ•°æ®é¢„å¤„ç†é»„é‡‘å‡†åˆ™:
        1. åˆ†å‰²å‡ºå¹²å‡€çš„è®­ç»ƒ/æµ‹è¯•é›†
        2. åœ¨ã€å¹²å‡€ã€‘çš„y_trainä¸Šæ‹ŸåˆStandardScaler
        3. åœ¨åŸå§‹å°ºåº¦çš„y_trainä¸Šæ³¨å…¥shuffleå™ªå£°
        4. ä½¿ç”¨ã€å¹²å‡€ã€‘çš„scaleræ¥è½¬æ¢å¸¦å™ªå£°çš„y_train
        5. æ‰€æœ‰æ¨¡å‹éƒ½åœ¨å®Œå…¨æ ‡å‡†åŒ–çš„(X, y)ç©ºé—´ä¸­è®­ç»ƒ
        6. æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹ç»“æœéƒ½é€†è½¬æ¢ä¸ºåŸå§‹å°ºåº¦è¿›è¡Œè¯„ä¼°
        """
        # 1. æ•°æ®åˆ†å‰²
        stratify_option = y if task_type == 'classification' else None
        X_train_full, X_test, y_train_full, y_test = train_test_split(
            X, y,
            test_size=test_size,
            random_state=random_state,
            stratify=stratify_option
        )
        if verbose:
            print(f"ğŸ”¥ æ•°æ®å‡†å¤‡: åˆ†å‰²æ•°æ®é›†...")
            print(f"   - åŸå§‹è®­ç»ƒé›†: {len(X_train_full)}, åŸå§‹æµ‹è¯•é›†: {len(X_test)}")

        # 2. å™ªå£°æ³¨å…¥ï¼ˆåœ¨åŸå§‹å°ºåº¦ä¸Šï¼‰
        y_train_noisy = y_train_full
        if anomaly_ratio > 0:
            y_train_noisy, noise_indices = inject_shuffle_noise(
                y_train_full,
                noise_ratio=anomaly_ratio,
                random_state=random_state
            )
            if verbose:
                print(f"   - æ³¨å…¥ {anomaly_ratio:.1%} shuffleå™ªå£°: {len(noise_indices)}ä¸ªæ ·æœ¬å—å½±å“")

        # 3. æ ‡å‡†åŒ–ï¼ˆéµå¾ªé»„é‡‘å‡†åˆ™ï¼‰
        scaler_X = StandardScaler()
        X_train_full_scaled = scaler_X.fit_transform(X_train_full)
        X_test_scaled = scaler_X.transform(X_test)

        scaler_y = None
        y_train_for_model = y_train_noisy
        if task_type == 'regression':
            scaler_y = StandardScaler()
            scaler_y.fit(y_train_full.reshape(-1, 1)) # åœ¨å¹²å‡€çš„yä¸Šæ‹Ÿåˆ
            y_train_for_model = scaler_y.transform(y_train_noisy.reshape(-1, 1)).flatten() # è½¬æ¢å¸¦å™ªçš„y
            if verbose:
                print(f"   - Xå’Œyéƒ½å·²æ ‡å‡†åŒ– (yåœ¨å¹²å‡€æ•°æ®ä¸Šæ‹Ÿåˆscaler)")
        else:
             if verbose:
                print(f"   - ä»…Xè¢«æ ‡å‡†åŒ– (åˆ†ç±»ä»»åŠ¡)")

        # 4. ä»ï¼ˆå¯èƒ½å¸¦å™ªçš„ï¼‰è®­ç»ƒé›†ä¸­åˆ†å‰²å‡ºéªŒè¯é›†
        stratify_val_option = y_train_noisy if task_type == 'classification' else None
        X_train, X_val, y_train, y_val = train_test_split(
            X_train_full_scaled, y_train_for_model,
            test_size=val_size,
            random_state=random_state,
            stratify=stratify_val_option
        )
        if verbose:
            print(f"   - æœ€ç»ˆè®­ç»ƒé›†: {len(X_train)}, éªŒè¯é›†: {len(X_val)}")
        
        results = {}
        
        # 5. ç¡®å®šè¦ä½¿ç”¨çš„åŸºå‡†æ–¹æ³•
        baseline_methods = self._get_baseline_methods(task_type, **kwargs)
        causal_modes = kwargs.get('causal_modes', ['deterministic', 'standard'])
        
        if verbose:
            print(f"\nğŸ“Š é€‰æ‹©çš„åŸºå‡†æ–¹æ³•: {baseline_methods}")
            print(f"ğŸ§  CausalEngineæ¨¡å¼: {causal_modes}")

        all_methods_to_run = baseline_methods + causal_modes
        
        # 6. ç»Ÿä¸€çš„è®­ç»ƒå’Œè¯„ä¼°å¾ªç¯
        for method_name in all_methods_to_run:
            if verbose:
                print("-" * 50)
                print(f"ğŸš€ æ­£åœ¨å¤„ç†: {method_name}")

            model = None
            try:
                if method_name in causal_modes:
                    causal_kwargs = {k: v for k, v in kwargs.items() if k in [
                        'hidden_sizes', 'max_epochs', 'lr', 'patience', 'tol',
                        'gamma_init', 'b_noise_init', 'b_noise_trainable', 'ovr_threshold'
                    ]}
                    model = self.train_causal_engine(
                        X_train, y_train, X_val, y_val, task_type, method_name, verbose=verbose, **causal_kwargs
                    )
                else:
                    method_config = DEFAULT_METHOD_CONFIGS.get(method_name)
                    if not method_config:
                        print(f"âŒ æœªçŸ¥æ–¹æ³•: {method_name}ï¼Œè·³è¿‡")
                        continue
                    
                    params = method_config['params'].copy()
                    if 'baseline_config' in kwargs:
                        config = kwargs['baseline_config']
                        if isinstance(config, dict) and 'method_params' in config:
                            user_params = config['method_params'].get(method_name, {})
                            params.update(user_params)
                    
                    model = self.method_factory.create_model(method_name, task_type, **params)
                    model.fit(X_train, y_train)
            
            except Exception as e:
                if verbose:
                    print(f"âŒ è®­ç»ƒ {method_name} æ—¶å‡ºé”™: {str(e)}")
                continue

            # ç»Ÿä¸€è¯„ä¼°
            if model:
                # å¯¹äºCausalEngineæ¨¡å‹ï¼Œéœ€è¦ç¡®ä¿è¾“å…¥æ˜¯torch tensor
                if method_name in causal_modes:
                    import torch
                    device = next(model.parameters()).device
                    X_test_torch = torch.FloatTensor(X_test_scaled).to(device)
                    X_val_torch = torch.FloatTensor(X_val).to(device)
                    
                    model.eval()
                    with torch.no_grad():
                        pred_test_scaled = model.predict(X_test_torch).cpu().numpy()
                        pred_val_scaled = model.predict(X_val_torch).cpu().numpy()
                else:
                    pred_test_scaled = model.predict(X_test_scaled)
                    pred_val_scaled = model.predict(X_val)

                if task_type == 'regression' and scaler_y:
                    pred_test = scaler_y.inverse_transform(pred_test_scaled.reshape(-1, 1)).flatten()
                    pred_val = scaler_y.inverse_transform(pred_val_scaled.reshape(-1, 1)).flatten()
                    y_val_original = scaler_y.inverse_transform(y_val.reshape(-1, 1)).flatten()
                    
                    y_test_original = y_test # æµ‹è¯•é›†yå§‹ç»ˆæ˜¯å¹²å‡€ã€åŸå§‹çš„
                    
                    results[method_name] = {
                        'test': {'MAE': mean_absolute_error(y_test_original, pred_test),
                                 'MdAE': median_absolute_error(y_test_original, pred_test),
                                 'RMSE': np.sqrt(mean_squared_error(y_test_original, pred_test)),
                                 'RÂ²': r2_score(y_test_original, pred_test)},
                        'val': {'MAE': mean_absolute_error(y_val_original, pred_val),
                                'MdAE': median_absolute_error(y_val_original, pred_val),
                                'RMSE': np.sqrt(mean_squared_error(y_val_original, pred_val)),
                                'RÂ²': r2_score(y_val_original, pred_val)}
                    }
                else:
                    n_classes = len(np.unique(y_train_full))
                    avg_method = 'binary' if n_classes == 2 else 'macro'
                    
                    results[method_name] = {
                        'test': {'Acc': accuracy_score(y_test, pred_test_scaled),
                                 'Precision': precision_score(y_test, pred_test_scaled, average=avg_method, zero_division=0),
                                 'Recall': recall_score(y_test, pred_test_scaled, average=avg_method, zero_division=0),
                                 'F1': f1_score(y_test, pred_test_scaled, average=avg_method, zero_division=0)},
                        'val': {'Acc': accuracy_score(y_val, pred_val_scaled),
                                'Precision': precision_score(y_val, pred_val_scaled, average=avg_method, zero_division=0),
                                'Recall': recall_score(y_val, pred_val_scaled, average=avg_method, zero_division=0),
                                'F1': f1_score(y_val, pred_val_scaled, average=avg_method, zero_division=0)}
                    }

        return results
    
    def format_results_table(self, results, task_type='regression'):
        """æ ¼å¼åŒ–ç»“æœä¸ºè¡¨æ ¼å­—ç¬¦ä¸²"""
        if task_type == 'regression':
            metrics = ['MAE', 'MdAE', 'RMSE', 'RÂ²']
            title = "ğŸ“Š å›å½’æ€§èƒ½å¯¹æ¯”"
        else:
            metrics = ['Acc', 'Precision', 'Recall', 'F1']
            title = "ğŸ“Š åˆ†ç±»æ€§èƒ½å¯¹æ¯”"
        
        lines = []
        lines.append(f"\n{title}")
        lines.append("=" * 120)
        lines.append(f"{'æ–¹æ³•':<25} {'éªŒè¯é›†':<50} {'æµ‹è¯•é›†':<50}")
        lines.append(f"{'':25} {metrics[0]:<10} {metrics[1]:<10} {metrics[2]:<10} {metrics[3]:<10} "
                    f"{metrics[0]:<10} {metrics[1]:<10} {metrics[2]:<10} {metrics[3]:<10}")
        lines.append("-" * 120)
        
        # è·å–æ‰€æœ‰æ–¹æ³•åç§°çš„é…ç½®
        method_configs = {key: val['name'] for key, val in DEFAULT_METHOD_CONFIGS.items()}

        for method, res_dict in sorted(results.items(), key=lambda item: item[1]['test'].get('MdAE', float('inf'))):
            display_name = method_configs.get(method, method.capitalize())
            val_m = res_dict.get('val', {})
            test_m = res_dict.get('test', {})
            
            val_values = [val_m.get(m, float('nan')) for m in metrics]
            test_values = [test_m.get(m, float('nan')) for m in metrics]
            
            lines.append(f"{display_name:<25} {val_values[0]:<10.4f} {val_values[1]:<10.4f} "
                        f"{val_values[2]:<10.4f} {val_values[3]:<10.4f} "
                        f"{test_values[0]:<10.4f} {test_values[1]:<10.4f} "
                        f"{test_values[2]:<10.4f} {test_values[3]:<10.4f}")
        
        lines.append("=" * 120)
        return '\n'.join(lines)
    
    def print_results(self, results, task_type='regression'):
        """æ‰“å°åŸºå‡†æµ‹è¯•ç»“æœ"""
        print(self.format_results_table(results, task_type))
    
    def benchmark_synthetic_data(self, task_type='regression', n_samples=1000, n_features=20, 
                                anomaly_ratio=0.0, verbose=True, **kwargs):
        """åœ¨åˆæˆæ•°æ®ä¸Šè¿›è¡ŒåŸºå‡†æµ‹è¯•"""
        from sklearn.datasets import make_regression, make_classification
        
        if task_type == 'regression':
            X, y = make_regression(
                n_samples=n_samples, 
                n_features=n_features, 
                noise=0.1, 
                random_state=kwargs.get('random_state', 42)
            )
        else:
            X, y = make_classification(
                n_samples=n_samples,
                n_features=n_features,
                n_informative=max(n_features//2, 2),
                n_redundant=0,
                n_clusters_per_class=1,
                random_state=kwargs.get('random_state', 42)
            )
        
        if verbose:
            print(f"\nğŸ§ª {task_type.title()} åŸºå‡†æµ‹è¯•")
            print(f"æ•°æ®é›†: {n_samples} æ ·æœ¬, {n_features} ç‰¹å¾")
            if anomaly_ratio > 0:
                print(f"æ ‡ç­¾å¼‚å¸¸: {anomaly_ratio:.1%}")
        
        results = self.compare_models(
            X, y, task_type=task_type, anomaly_ratio=anomaly_ratio, 
            verbose=verbose, **kwargs
        )
        
        if verbose:
            self.print_results(results, task_type)
        
        return results
    
    def _get_baseline_methods(self, task_type: str, **kwargs) -> list:
        """
        ç¡®å®šè¦ä½¿ç”¨çš„åŸºå‡†æ–¹æ³•åˆ—è¡¨
        """
        # æ–¹å¼1: ç›´æ¥æŒ‡å®šæ–¹æ³•åˆ—è¡¨
        if 'baseline_methods' in kwargs:
            methods = kwargs['baseline_methods']
            if isinstance(methods, str):
                methods = [methods]
            
            methods = expand_method_groups(methods)
            available_methods, _ = filter_available_methods(methods)
            return available_methods
        
        # æ–¹å¼2: ä½¿ç”¨é¢„å®šä¹‰æ–¹æ³•ç»„åˆ
        if 'method_group' in kwargs:
            group_name = kwargs['method_group']
            methods = get_method_group(group_name)
            available_methods, _ = filter_available_methods(methods)
            return available_methods
        
        # æ–¹å¼3: ä»»åŠ¡ç‰¹å®šæ¨è
        if 'recommendation_type' in kwargs:
            rec_type = kwargs['recommendation_type']
            methods = get_task_recommendations(task_type, rec_type)
            available_methods, _ = filter_available_methods(methods)
            return available_methods
        
        # é»˜è®¤æ–¹å¼ï¼šå‘åå…¼å®¹
        return ['sklearn_mlp', 'pytorch_mlp']
    
    def list_available_baseline_methods(self) -> dict:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„åŸºå‡†æ–¹æ³•"""
        all_methods = list_available_methods()
        available = {}
        
        for method in all_methods:
            config = DEFAULT_METHOD_CONFIGS.get(method)
            available[method] = {
                'name': config['name'],
                'type': config['type'],
                'available': self.method_factory.is_method_available(method)
            }
        
        return available
    
    def print_method_availability(self):
        """æ‰“å°æ–¹æ³•å¯ç”¨æ€§æŠ¥å‘Š"""
        print("\nğŸ“¦ åŸºå‡†æ–¹æ³•å¯ç”¨æ€§æŠ¥å‘Š")
        print("=" * 80)
        
        methods = self.list_available_baseline_methods()
        
        by_type = {}
        for method, info in methods.items():
            method_type = info['type']
            if method_type not in by_type:
                by_type[method_type] = []
            by_type[method_type].append((method, info))
        
        for method_type, method_list in by_type.items():
            print(f"\nğŸ“Š {method_type.title()} Methods:")
            print("-" * 40)
            
            for method, info in method_list:
                status = "âœ…" if info['available'] else "âŒ"
                print(f"  {status} {method:<20} - {info['name']}")
        
        self.dependency_checker.print_dependency_status()