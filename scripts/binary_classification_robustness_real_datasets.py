#!/usr/bin/env python3
"""
äºŒåˆ†ç±»ç®—æ³•çœŸå®æ•°æ®é›†å™ªå£°é²æ£’æ€§æµ‹è¯•è„šæœ¬

ğŸ¯ ç›®æ ‡ï¼šåœ¨çœŸå®äºŒåˆ†ç±»æ•°æ®é›†ä¸Šæµ‹è¯• OvR å’Œ single_score æ¨¡å¼çš„å™ªå£°é²æ£’æ€§
ğŸ”¬ æ ¸å¿ƒï¼šåŸºäº classification_robustness_real_datasets.pyï¼Œä»…ä¿®æ”¹ä¸ºæµ‹è¯•äºŒåˆ†ç±»

ä¸»è¦ç‰¹æ€§ï¼š
- çœŸå®æ•°æ®é›†ï¼šBreast Cancer (äºŒåˆ†ç±»æ•°æ®é›†)
- ç®—æ³•å¯¹æ¯”ï¼šsklearn MLP, CausalEngine OvR, CausalEngine single_score
- å™ªå£°çº§åˆ«ï¼š0%, 10%, 20%, ..., 100% (11ä¸ªçº§åˆ«)
- å®Œæ•´æŒ‡æ ‡ï¼šAccuracy, Precision, Recall, F1
- æŠ˜çº¿å›¾å¯è§†åŒ–ï¼šæ¸…æ™°å±•ç¤ºç®—æ³•åœ¨çœŸå®æ•°æ®ä¸Šçš„é²æ£’æ€§å¯¹æ¯”

ä½¿ç”¨æ–¹æ³•ï¼š
python scripts/binary_classification_robustness_real_datasets.py
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.datasets import load_breast_cancer, fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import os
import sys
import warnings
from tqdm import tqdm
import pandas as pd

# è®¾ç½®matplotlibåç«¯ï¼Œé¿å…å¼¹å‡ºçª—å£
plt.switch_backend('Agg')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥åˆ†ç±»å™¨
from causal_sklearn.classifier import MLPCausalClassifier
from causal_sklearn.data_processing import inject_shuffle_noise

warnings.filterwarnings('ignore')

# =============================================================================
# é…ç½®éƒ¨åˆ† - åœ¨è¿™é‡Œä¿®æ”¹å®éªŒå‚æ•°
# =============================================================================

BINARY_ROBUSTNESS_CONFIG = {
    # æ•°æ®é›†é€‰æ‹© - å¯é€‰å¤šä¸ªäºŒåˆ†ç±»æ•°æ®é›†
    'dataset': 'spambase',  # å¯é€‰: 'breast_cancer', 'adult', 'bank_marketing', 'spambase'
    
    # å™ªå£°çº§åˆ«è®¾ç½®
    'noise_levels': np.linspace(0, 1, 6),  # 0%, 10%, 20%, ..., 100%
    
    # æ•°æ®å‚æ•°
    'random_state': 42,     # å›ºå®šéšæœºç§å­
    'test_size': 0.2,       # æµ‹è¯•é›†æ¯”ä¾‹
    
    # ç½‘ç»œç»“æ„ï¼ˆæ‰€æœ‰ç®—æ³•ç»Ÿä¸€ï¼‰
    'hidden_layers': (128, 64, 64),      # ä¿æŒç½‘ç»œç»“æ„
    'max_iter': 3000,               # æœ€å¤§è¿­ä»£æ¬¡æ•°
    'learning_rate': 0.001,         # å­¦ä¹ ç‡
    'patience': 100,                # æ—©åœè€å¿ƒ
    'tol': 1e-4,                    # æ”¶æ•›å®¹å¿åº¦
    'batch_size': None,             # æ‰¹å¤„ç†å¤§å°
    
    # ç¨³å®šæ€§æ”¹è¿›å‚æ•°
    'n_runs': 2,                     # 5æ¬¡è¿è¡Œ
    'base_random_seed': 42,          # åŸºç¡€éšæœºç§å­
    
    # é¢å¤–ç¨³å®šæ€§å‚æ•°
    'validation_fraction': 0.2,     # éªŒè¯é›†æ¯”ä¾‹ï¼ˆæ—©åœç”¨ï¼‰
    'early_stopping': True,          # ç¡®ä¿æ—©åœå¼€å¯
    
    # è¾“å‡ºæ§åˆ¶
    'output_dir': 'results/binary_classification_robustness',
    'save_plots': True,
    'save_data': True,
    'verbose': True,
    'figure_dpi': 300
}

# =============================================================================
# å·¥å…·å‡½æ•°
# =============================================================================

def _ensure_output_dir(output_dir):
    """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)

def _get_output_path(output_dir, filename):
    """è·å–è¾“å‡ºæ–‡ä»¶è·¯å¾„"""
    return os.path.join(output_dir, filename)

def load_binary_dataset(dataset_name='breast_cancer', max_samples=10000):
    """åŠ è½½äºŒåˆ†ç±»æ•°æ®é›†
    
    æ”¯æŒçš„æ•°æ®é›†ï¼š
    - breast_cancer: ä¹³è…ºç™Œæ£€æµ‹ï¼ˆç®€å•ï¼Œå¹²å‡€ï¼‰
    - adult: æˆäººæ”¶å…¥é¢„æµ‹ï¼ˆä¸­ç­‰éš¾åº¦ï¼Œæœ‰ç¼ºå¤±å€¼ï¼‰
    - bank_marketing: é“¶è¡Œè¥é”€ï¼ˆä¸å¹³è¡¡ï¼‰
    - spambase: åƒåœ¾é‚®ä»¶æ£€æµ‹ï¼ˆé«˜ç»´ï¼‰
    """
    
    if dataset_name == 'breast_cancer':
        data = load_breast_cancer()
        X, y = data.data, data.target
        desc = "569 samples, 30 features, 2 classes (balanced, clean)"
        
    elif dataset_name == 'adult':
        # Adult dataset - æ”¶å…¥é¢„æµ‹
        print("ğŸ“¥ ä» OpenML åŠ è½½ Adult æ•°æ®é›†...")
        try:
            data = fetch_openml('adult', version=2, as_frame=True, parser='auto')
            X = data.data
            y = (data.target == '>50K').astype(int)  # è½¬ä¸ºäºŒåˆ†ç±»
            
            # å¤„ç†åˆ†ç±»ç‰¹å¾
            from sklearn.preprocessing import LabelEncoder
            categorical_cols = X.select_dtypes(include=['object', 'category']).columns
            X_encoded = X.copy()
            for col in categorical_cols:
                le = LabelEncoder()
                # å…ˆè½¬æ¢ä¸ºå­—ç¬¦ä¸²å¤„ç†ç¼ºå¤±å€¼
                X_encoded[col] = X_encoded[col].astype(str).fillna('missing')
                X_encoded[col] = le.fit_transform(X_encoded[col])
            X = X_encoded
            
            X = X.values
            y = y.values
            
            # é™åˆ¶æ ·æœ¬æ•°
            if len(X) > max_samples:
                idx = np.random.choice(len(X), max_samples, replace=False)
                X, y = X[idx], y[idx]
            
            desc = f"{len(X)} samples, {X.shape[1]} features, imbalanced (~24% positive)"
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {e}")
            print("ğŸ“‹ ä½¿ç”¨å¤‡ç”¨çš„ breast_cancer æ•°æ®é›†")
            return load_binary_dataset('breast_cancer')
    
    elif dataset_name == 'bank_marketing':
        # Bank Marketing dataset - é“¶è¡Œè¥é”€
        print("ğŸ“¥ ä» OpenML åŠ è½½ Bank Marketing æ•°æ®é›†...")
        try:
            data = fetch_openml('bank-marketing', version=1, as_frame=True, parser='auto')
            X = data.data
            y = (data.target == '2').astype(int)  # '2' è¡¨ç¤ºè®¢é˜…
            
            # å¤„ç†åˆ†ç±»ç‰¹å¾
            from sklearn.preprocessing import LabelEncoder
            categorical_cols = X.select_dtypes(include=['object', 'category']).columns
            X_encoded = X.copy()
            for col in categorical_cols:
                le = LabelEncoder()
                # å…ˆè½¬æ¢ä¸ºå­—ç¬¦ä¸²å¤„ç†ç¼ºå¤±å€¼
                X_encoded[col] = X_encoded[col].astype(str).fillna('missing')
                X_encoded[col] = le.fit_transform(X_encoded[col])
            X = X_encoded
            
            X = X.values
            y = y.values
            
            # é™åˆ¶æ ·æœ¬æ•°
            if len(X) > max_samples:
                idx = np.random.choice(len(X), max_samples, replace=False)
                X, y = X[idx], y[idx]
            
            desc = f"{len(X)} samples, {X.shape[1]} features, highly imbalanced (~11% positive)"
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {e}")
            print("ğŸ“‹ ä½¿ç”¨å¤‡ç”¨çš„ breast_cancer æ•°æ®é›†")
            return load_binary_dataset('breast_cancer')
    
    elif dataset_name == 'spambase':
        # Spambase dataset - åƒåœ¾é‚®ä»¶æ£€æµ‹
        print("ğŸ“¥ ä» OpenML åŠ è½½ Spambase æ•°æ®é›†...")
        try:
            data = fetch_openml('spambase', version=1, as_frame=True, parser='auto')
            X = data.data.values
            y = (data.target == '1').astype(int)  # '1' è¡¨ç¤ºåƒåœ¾é‚®ä»¶
            
            desc = f"{len(X)} samples, {X.shape[1]} features, slightly imbalanced (~39% spam)"
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {e}")
            print("ğŸ“‹ ä½¿ç”¨å¤‡ç”¨çš„ breast_cancer æ•°æ®é›†")
            return load_binary_dataset('breast_cancer')
    
    else:
        print(f"âŒ æœªçŸ¥æ•°æ®é›†: {dataset_name}")
        print("ğŸ“‹ ä½¿ç”¨é»˜è®¤çš„ breast_cancer æ•°æ®é›†")
        return load_binary_dataset('breast_cancer')
    
    print(f"ğŸ“Š åŠ è½½æ•°æ®é›†: {dataset_name}")
    print(f"ğŸ“‹ æ•°æ®é›†æè¿°: {desc}")
    print(f"ğŸ“ å®é™…æ•°æ®å½¢çŠ¶: {X.shape}, ç±»åˆ«åˆ†å¸ƒ: {np.bincount(y)}")
    print(f"ğŸ“Š æ­£ç±»æ¯”ä¾‹: {np.mean(y):.2%}")
    
    return X, y

# =============================================================================
# äºŒåˆ†ç±»é²æ£’æ€§æµ‹è¯•
# =============================================================================

def test_binary_classification_noise_robustness(config):
    """æµ‹è¯•äºŒåˆ†ç±»ç®—æ³•çš„å™ªå£°é²æ£’æ€§"""
    print("\n" + "="*80)
    print("ğŸ¯ äºŒåˆ†ç±»ç®—æ³•å™ªå£°é²æ£’æ€§æµ‹è¯•")
    print("="*80)
    
    noise_levels = config['noise_levels']
    results = {}
    
    # å®šä¹‰è¦æµ‹è¯•çš„ç®—æ³•
    algorithms = {
        'sklearn_mlp': ('sklearn MLP', None, None),
        'causal_ovr_det': ('CausalEngine OvR (det)', 'ovr', 'deterministic'),
        'causal_ovr_std': ('CausalEngine OvR (std)', 'ovr', 'standard'),
        'causal_single_det': ('CausalEngine Single (det)', 'single_score', 'deterministic'),
        'causal_single_std': ('CausalEngine Single (std)', 'single_score', 'standard')
    }
    
    # åˆå§‹åŒ–ç»“æœå­—å…¸
    for algo_key, (algo_name, _, _) in algorithms.items():
        results[algo_key] = {
            'name': algo_name,
            'noise_levels': [],
            'accuracy': [], 'precision': [], 'recall': [], 'f1': []
        }
    
    # åŠ è½½æ•°æ®é›†
    X, y = load_binary_dataset(config.get('dataset', 'breast_cancer'))
    
    # åˆ†å‰²æ•°æ®
    X_train, X_test, y_train_clean, y_test = train_test_split(
        X, y, test_size=config['test_size'], random_state=config['random_state']
    )
    
    # æ ‡å‡†åŒ–ç‰¹å¾
    scaler_X = StandardScaler()
    X_train_scaled = scaler_X.fit_transform(X_train)
    X_test_scaled = scaler_X.transform(X_test)
    
    # åœ¨ä¸åŒå™ªå£°çº§åˆ«ä¸‹æµ‹è¯•
    for noise_level in tqdm(noise_levels, desc="å™ªå£°çº§åˆ«"):
        print(f"\nğŸ“Š æµ‹è¯•å™ªå£°çº§åˆ«: {noise_level:.1%}")
        
        # å¯¹è®­ç»ƒæ ‡ç­¾æ³¨å…¥å™ªå£°
        if noise_level > 0:
            y_train_noisy, noise_indices = inject_shuffle_noise(
                y_train_clean,
                noise_ratio=noise_level,
                random_state=config['random_state']
            )
        else:
            y_train_noisy = y_train_clean.copy()
        
        # æµ‹è¯•æ¯ä¸ªç®—æ³•
        for algo_key, (algo_name, binary_mode, causal_mode) in algorithms.items():
            try:
                if config['verbose']:
                    print(f"  ğŸ”§ è®­ç»ƒ {algo_name}...")
                
                # åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
                if algo_key == 'sklearn_mlp':
                    model = MLPClassifier(
                        hidden_layer_sizes=config['hidden_layers'],
                        max_iter=config['max_iter'],
                        learning_rate_init=config['learning_rate'],
                        random_state=config['random_state'],
                        early_stopping=config['early_stopping'],
                        validation_fraction=config['validation_fraction'],
                        n_iter_no_change=config['patience'],
                        tol=config['tol']
                    )
                    model.fit(X_train_scaled, y_train_noisy)
                    
                else:  # CausalEngine variants
                    model = MLPCausalClassifier(
                        perception_hidden_layers=config['hidden_layers'],
                        binary_mode=binary_mode,
                        mode=causal_mode,
                        max_iter=config['max_iter'],
                        learning_rate=config['learning_rate'],
                        batch_size=config['batch_size'],
                        random_state=config['random_state'],
                        early_stopping=config['early_stopping'],
                        validation_fraction=config['validation_fraction'],
                        verbose=False
                    )
                    model.fit(X_train_scaled, y_train_noisy)
                
                # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
                y_pred = model.predict(X_test_scaled)
                
                # è®¡ç®—æŒ‡æ ‡
                accuracy = accuracy_score(y_test, y_pred)
                precision = precision_score(y_test, y_pred, average='binary', zero_division=0)
                recall = recall_score(y_test, y_pred, average='binary', zero_division=0)
                f1 = f1_score(y_test, y_pred, average='binary', zero_division=0)
                
                # å­˜å‚¨ç»“æœ
                results[algo_key]['noise_levels'].append(noise_level)
                results[algo_key]['accuracy'].append(accuracy)
                results[algo_key]['precision'].append(precision)
                results[algo_key]['recall'].append(recall)
                results[algo_key]['f1'].append(f1)
                
                if config['verbose']:
                    print(f"    Acc: {accuracy:.4f}, F1: {f1:.4f}")
                
            except Exception as e:
                print(f"    âŒ {algo_name} è®­ç»ƒå¤±è´¥: {str(e)}")
                # æ·»åŠ NaNå€¼ä¿æŒæ•°ç»„é•¿åº¦ä¸€è‡´
                results[algo_key]['noise_levels'].append(noise_level)
                results[algo_key]['accuracy'].append(np.nan)
                results[algo_key]['precision'].append(np.nan)
                results[algo_key]['recall'].append(np.nan)
                results[algo_key]['f1'].append(np.nan)
    
    return results

# =============================================================================
# å¯è§†åŒ–å‡½æ•°
# =============================================================================

def create_binary_robustness_plots(results, config):
    """åˆ›å»ºäºŒåˆ†ç±»é²æ£’æ€§åˆ†ææŠ˜çº¿å›¾"""
    if not config.get('save_plots', False):
        return
    
    _ensure_output_dir(config['output_dir'])
    
    print("\nğŸ“Š åˆ›å»ºäºŒåˆ†ç±»é²æ£’æ€§åˆ†æå›¾è¡¨")
    print("-" * 50)
    
    # è®¾ç½®å›¾è¡¨é£æ ¼
    plt.style.use('seaborn-v0_8')
    colors = plt.cm.Set3(np.linspace(0, 1, 12))  # 12ç§ä¸åŒé¢œè‰²
    
    # åˆ›å»ºåˆ†ç±»é²æ£’æ€§å›¾è¡¨
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    n_runs = config.get('n_runs', 1)
    title_suffix = f' (Averaged over {n_runs} runs)' if n_runs > 1 else ''
    fig.suptitle(f'Binary Classification Algorithms Noise Robustness{title_suffix}', 
                 fontsize=16, fontweight='bold')
    
    metrics = ['accuracy', 'precision', 'recall', 'f1']
    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1']
    
    for i, (metric, metric_name) in enumerate(zip(metrics, metric_names)):
        ax = axes[i//2, i%2]
        
        color_idx = 0
        for algo_key, data in results.items():
            if data[metric]:  # ç¡®ä¿æœ‰æ•°æ®
                noise_levels = np.array(data['noise_levels']) * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
                values = np.array(data[metric])
                
                # è¿‡æ»¤NaNå€¼
                valid_mask = ~np.isnan(values)
                if valid_mask.any():
                    # åˆ¤æ–­æ˜¯å¦ä¸ºå› æœç®—æ³•
                    is_causal = algo_key.startswith('causal_')
                    is_single = 'single' in algo_key
                    
                    # è®¾ç½®çº¿å‹ï¼šsklearnè™šçº¿ï¼Œcausal OvRç»†å®çº¿ï¼Œcausal singleç²—å®çº¿
                    if not is_causal:
                        linestyle = '--'
                        linewidth = 2
                    elif is_single:
                        linestyle = '-'
                        linewidth = 3  # ç²—çº¿çªå‡ºsingle_score
                    else:
                        linestyle = '-'
                        linewidth = 2
                    
                    ax.plot(noise_levels[valid_mask], values[valid_mask], 
                           marker='o', linewidth=linewidth, markersize=4, linestyle=linestyle,
                           label=data['name'], color=colors[color_idx])
                    color_idx += 1
        
        ax.set_xlabel('Noise Level (%)')
        ax.set_ylabel(metric_name)
        ax.set_title(f'{metric_name} vs Noise Level')
        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        ax.grid(True, alpha=0.3)
        
        # åŠ¨æ€è®¾ç½®yè½´èŒƒå›´ä»¥æ›´å¥½åœ°å±•ç¤ºå·®å¼‚
        # è·å–æ‰€æœ‰æœ‰æ•ˆæ•°æ®çš„æœ€å°å€¼
        all_valid_values = []
        for algo_key, data in results.items():
            if data[metric]:
                values = np.array(data[metric])
                valid_values = values[~np.isnan(values)]
                if len(valid_values) > 0:
                    all_valid_values.extend(valid_values)
        
        if all_valid_values:
            min_value = min(all_valid_values)
            # å¦‚æœæœ€å°å€¼å¤§äº0.5ï¼Œä»0.5å¼€å§‹ï¼›å¦åˆ™ç•™ä¸€äº›è¾¹è·
            if min_value > 0.5:
                y_min = 0.5
            else:
                y_min = max(0, min_value - 0.05)
            ax.set_ylim(y_min, 1.05)
    
    plt.tight_layout()
    
    # ç”Ÿæˆæ–‡ä»¶åï¼ŒåŠ¨æ€åŒ…å«æ•°æ®é›†åç§°
    dataset_key = config.get('dataset', 'unknown_dataset')
    plot_filename = f'binary_classification_robustness_{dataset_key}.png'
    plot_path = _get_output_path(config['output_dir'], plot_filename)
    
    plt.savefig(plot_path, dpi=config['figure_dpi'], bbox_inches='tight')
    print(f"ğŸ“Š äºŒåˆ†ç±»é²æ£’æ€§å›¾è¡¨å·²ä¿å­˜ä¸º {plot_path}")
    plt.close()

# =============================================================================
# ä¸»å‡½æ•°
# =============================================================================

def run_single_binary_robustness_analysis(config, run_idx=0):
    """è¿è¡Œå•æ¬¡äºŒåˆ†ç±»é²æ£’æ€§åˆ†æ"""
    if config['verbose']:
        print(f"\nğŸ”„ ç¬¬ {run_idx + 1}/{config['n_runs']} æ¬¡è¿è¡Œ (éšæœºç§å­: {config['random_state']})")
    
    # è¿è¡ŒäºŒåˆ†ç±»é²æ£’æ€§æµ‹è¯•
    results = test_binary_classification_noise_robustness(config)
    
    return results

def aggregate_binary_results(all_results):
    """èšåˆå¤šæ¬¡è¿è¡Œçš„äºŒåˆ†ç±»ç»“æœ"""
    aggregated_results = {}
    
    if all_results and len(all_results) > 0:
        first_result = all_results[0]
        if first_result:
            for algo_key in first_result.keys():
                aggregated_results[algo_key] = {
                    'name': first_result[algo_key]['name'],
                    'noise_levels': first_result[algo_key]['noise_levels'],
                    'accuracy': [], 'precision': [], 'recall': [], 'f1': [],
                    'accuracy_std': [], 'precision_std': [], 'recall_std': [], 'f1_std': []
                }
                
                # æ”¶é›†æ‰€æœ‰è¿è¡Œçš„ç»“æœ
                metrics = ['accuracy', 'precision', 'recall', 'f1']
                for metric in metrics:
                    all_values = []
                    for run_result in all_results:
                        if run_result and algo_key in run_result:
                            all_values.append(run_result[algo_key][metric])
                    
                    if all_values:
                        # è®¡ç®—æ¯ä¸ªå™ªå£°çº§åˆ«çš„å‡å€¼å’Œæ ‡å‡†å·®
                        all_values = np.array(all_values)  # shape: (n_runs, n_noise_levels)
                        means = np.nanmean(all_values, axis=0)
                        stds = np.nanstd(all_values, axis=0)
                        
                        aggregated_results[algo_key][metric] = means.tolist()
                        aggregated_results[algo_key][f'{metric}_std'] = stds.tolist()
    
    return aggregated_results

def run_binary_classification_robustness_analysis(config=None):
    """è¿è¡Œå®Œæ•´çš„å¤šæ¬¡äºŒåˆ†ç±»é²æ£’æ€§åˆ†æ"""
    if config is None:
        config = BINARY_ROBUSTNESS_CONFIG
    
    print("ğŸš€ äºŒåˆ†ç±»ç®—æ³•å™ªå£°é²æ£’æ€§åˆ†æ")
    print("=" * 70)
    print(f"æ•°æ®é›†: {config.get('dataset', 'breast_cancer')}")
    print(f"å™ªå£°çº§åˆ«: {config['noise_levels'][0]:.0%} - {config['noise_levels'][-1]:.0%} ({len(config['noise_levels'])}ä¸ªçº§åˆ«)")
    print(f"è¿è¡Œæ¬¡æ•°: {config['n_runs']}æ¬¡ (éšæœºç§å­: {config['base_random_seed']} - {config['base_random_seed'] + config['n_runs'] - 1})")
    
    all_results = []
    
    # å¤šæ¬¡è¿è¡Œ
    for run_idx in range(config['n_runs']):
        # ä¸ºæ¯æ¬¡è¿è¡Œè®¾ç½®ä¸åŒçš„éšæœºç§å­
        run_config = config.copy()
        run_config['random_state'] = config['base_random_seed'] + run_idx
        
        result = run_single_binary_robustness_analysis(run_config, run_idx)
        all_results.append(result)
    
    # èšåˆç»“æœ
    print(f"\nğŸ“Š èšåˆ {config['n_runs']} æ¬¡è¿è¡Œçš„ç»“æœ...")
    aggregated_results = aggregate_binary_results(all_results)
    
    # åˆ›å»ºå¯è§†åŒ–ï¼ˆä½¿ç”¨èšåˆåçš„ç»“æœï¼‰
    create_binary_robustness_plots(aggregated_results, config)
    
    # ä¿å­˜ç»“æœæ•°æ®
    if config.get('save_data', True):
        _ensure_output_dir(config['output_dir'])
        
        # åŠ¨æ€ç”Ÿæˆæ•°æ®æ–‡ä»¶å
        dataset_key = config.get('dataset', 'unknown_dataset')
        data_filename = f'binary_classification_results_{dataset_key}_aggregated.npy'
        data_path = _get_output_path(config['output_dir'], data_filename)
        
        np.save(data_path, aggregated_results)
        print(f"ğŸ“Š èšåˆç»“æœå·²ä¿å­˜ä¸º {data_path}")
    
    print(f"\nâœ… äºŒåˆ†ç±»é²æ£’æ€§åˆ†æå®Œæˆ!")
    print(f"ğŸ“Š ç»“æœä¿å­˜åœ¨: {config['output_dir']}")
    
    return aggregated_results

# =============================================================================
# å…¥å£ç‚¹
# =============================================================================

if __name__ == '__main__':
    # è¿è¡ŒäºŒåˆ†ç±»é²æ£’æ€§åˆ†æ
    results = run_binary_classification_robustness_analysis()