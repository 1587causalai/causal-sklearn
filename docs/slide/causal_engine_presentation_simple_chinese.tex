%!TEX program = xelatex
\documentclass[aspectratio=169,12pt]{beamer}
\usepackage{xeCJK}

% 手动设置中文字体（适用于 Mac）
\setCJKmainfont{PingFang SC}
\setCJKsansfont{PingFang SC}
\setCJKmonofont{PingFang SC}
\usepackage{fontspec}
% \newfontfamily{\emojifont}[Renderer=Full]{Apple Color Emoji}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows.meta}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}

% 主题设置
\usetheme{Madrid}
\usecolortheme{default}
\usefonttheme{professionalfonts}

% 代码高亮设置
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{orange},
    showstringspaces=false,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10}
}

% 标题信息
\title{因果引擎™：让机器学习更智能}
\subtitle{从相关性到因果性的革命}
\author{龚鹤阳}
\date{\today}

\begin{document}

% 标题页
\begin{frame}
\titlepage
\end{frame}

% 目录
\begin{frame}{今天我们要讲什么？}
\tableofcontents
\end{frame}

% 第一部分：问题是什么？
\section{传统机器学习有什么问题？}

\begin{frame}{传统机器学习的局限性}
\begin{columns}
\column{0.5\textwidth}
\textbf{传统机器学习做什么？}
\begin{itemize}
    \item 学习数据中的相关性
    \item 就像：看到乌云就说要下雨
    \item 但不知道为什么下雨
\end{itemize}

\vspace{1em}
\textbf{问题出现了：}
\begin{itemize}
    \item 数据有噪音就不准了
    \item 每个人的差异被当作"干扰"
    \item 只能告诉你"是什么"，不知道"为什么"
\end{itemize}

\column{0.5\textwidth}
\begin{center}
\begin{tikzpicture}[scale=0.8]
% 传统ML示意图
\node[draw, fill=blue!20, rounded corners, minimum width=2cm, minimum height=1cm] (x) at (0,0) {输入};
\node[draw, fill=black!20, rounded corners, minimum width=2cm, minimum height=1cm] (box) at (3,0) {黑盒子};
\node[draw, fill=red!20, rounded corners, minimum width=2cm, minimum height=1cm] (y) at (6,0) {输出};
\draw[->, thick] (x) -- (box);
\draw[->, thick] (box) -- (y);
\node at (3,-1.5) {\small 不知道里面发生了什么};
\end{tikzpicture}
\end{center}
\end{columns}

\vspace{1em}
\begin{alertblock}{核心问题}
现实世界的数据总是有噪音的，传统方法应付不了！
\end{alertblock}
\end{frame}

\begin{frame}{我们需要什么样的解决方案？}
\begin{center}
\begin{tikzpicture}[scale=1.2]
% 对比图
\node at (0,3) {\textbf{\large 传统机器学习}};
\node at (6,3) {\textbf{\large 因果机器学习}};

% 传统方法
\node[draw, fill=blue!20, rounded corners] (x1) at (-1,1.5) {X};
\node[draw, fill=red!20, rounded corners] (y1) at (1,1.5) {Y};
\draw[->, thick] (x1) -- (y1) node[midway, above] {\small 相关性};

% 因果方法
\node[draw, fill=blue!20, rounded corners] (x2) at (5,1.5) {X};
\node[draw, circle, fill=yellow!30] (u) at (6,1.5) {U};
\node[draw, fill=red!20, rounded corners] (y2) at (7,1.5) {Y};
\draw[->, thick] (x2) -- (u);
\draw[->, thick] (u) -- (y2);
\node at (6,0.8) {\small 个体特征};

% 优势对比
\node[align=left] at (0,0) {\small • 简单直接\\• 容易过拟合\\• 噪音敏感};
\node[align=left] at (6,0) {\small • 理解机制\\• 抗噪能力强\\• 个性化预测};
\end{tikzpicture}
\end{center}

\begin{block}{关键洞察}
每个个体都有独特的特征U，这不是噪音，而是有用的信息！
\end{block}
\end{frame}

% 第二部分：因果引擎怎么工作？
\section{因果引擎™是怎么工作的？}

\begin{frame}{因果引擎™的四个步骤}
\begin{center}
\begin{tikzpicture}[
    scale=1,
    box/.style={draw, rounded corners=5pt, minimum width=1.8cm, minimum height=1cm, font=\small},
    arrow/.style={->, thick, >=Stealth}
]

% 主要流程
\node[left, font=\small] (X) {$\mathbf{X}$};
% \node[box, fill=blue!20] (X) at (0,0) {输入};
\node[box, fill=green!20] (P) at (2.5,0) {感知};
\node[box, fill=yellow!20] (Ab) at (5,0) {归因};
\node[box, fill=orange!20] (Ac) at (7.5,0) {推理};
\node[box, fill=red!20] (D) at (10,0) {决策};

% 箭头和中间变量
\draw[arrow] (X) -- (P);
\draw[arrow] (P) -- (Ab) node[midway, above, font=\small] {$\mathbf{Z}$};
\draw[arrow] (Ab) -- (Ac) node[midway, above, font=\small] {$\mathbf{U}$};
\draw[arrow] (Ac) -- (D) node[midway, above, font=\small] {$\mathbf{S}$};
\draw[arrow] (D) -- ++(1.5,0) node[right, font=\small] {$\mathbf{Y}$};

% 下方说明
\node[align=center, font=\footnotesize] at (2.5,-1.5) {提取特征\\像人看东西};
\node[align=center, font=\footnotesize] at (5,-1.5) {归因推断\\找到相似个体};
\node[align=center, font=\footnotesize] at (7.5,-1.5) {因果推理\\得到决策分数};
\node[align=center, font=\footnotesize] at (10,-1.5) {策略应用\\输出};

\end{tikzpicture}
\end{center}

\vspace{1em}
\begin{alertblock}{核心思想}
    就像医生看病：先观察症状，推断病因，再对症下药！
\end{alertblock}
\end{frame}

\begin{frame}{为什么选择柯西分布？}
\begin{columns}
\column{0.5\textwidth}
\textbf{柯西分布的特点：}
\begin{itemize}
    \item 有"重尾巴"—— 更好地处理极端情况
    \item 数学性质好—— 计算简单高效
    \item 更符合现实—— 人与人差异很大
\end{itemize}

\vspace{1em}
\textbf{打个比方：}
\begin{itemize}
    \item 正态分布：大部分人都差不多
    \item 柯西分布：承认有些人就是很特别
\end{itemize}

\column{0.5\textwidth}
\begin{center}
\begin{tikzpicture}[scale=0.8]
% 坐标轴
\draw[->] (-3,0) -- (3,0) node[right] {值};
\draw[->] (0,0) -- (0,2.5) node[above] {概率};

% 正态分布（窄一些）
\draw[red, thick, domain=-2.5:2.5, samples=100] 
    plot (\x, {2*exp(-\x*\x/0.5)});
\node[red] at (1.5, 1.5) {\small 正态分布};

% 柯西分布（宽一些）
\draw[blue, thick, domain=-2.5:2.5, samples=100] 
    plot (\x, {1.5/(1+\x*\x)});
\node[blue] at (-1.5, 0.8) {\small 柯西分布};

\node at (0,-0.8) {\footnotesize 柯西分布的"尾巴"更长};
\end{tikzpicture}
\end{center}
\end{columns}

\begin{block}{简单理解}
柯西分布让我们的模型能更好地理解"特殊"的个体
\end{block}
\end{frame}

% 第三部分：五种工作模式
\section{五种工作模式}

\begin{frame}{五种推理模式}
\begin{center}
\begin{tikzpicture}[scale=0.9]
% 画五个圆圈代表五种模式
\node[circle, draw, fill=gray!20, minimum size=1.5cm] (det) at (0,2) {\small 确定性};
\node[circle, draw, fill=blue!20, minimum size=1.5cm] (exo) at (-2,0) {\small 环境};
\node[circle, draw, fill=green!20, minimum size=1.5cm] (endo) at (2,0) {\small 认知};
\node[circle, draw, fill=red!30, minimum size=1.8cm] (std) at (0,-2) {\textbf{标准}};
\node[circle, draw, fill=yellow!20, minimum size=1.5cm] (samp) at (0,0) {\small 采样};

% 连线
\draw[dashed] (exo) -- (std);
\draw[dashed] (endo) -- (std);

% 说明
\node at (0,3.5) {\textbf{五种推理模式}};
\node[align=center] at (-4,0) {\small 环境噪音\\（天气、设备等）};
\node[align=center] at (4,0) {\small 认知不确定\\（个体差异）};
\node[align=center] at (0,-3.5) {\textbf{标准模式}\\结合两种不确定性};
\end{tikzpicture}
\end{center}

\begin{alertblock}{推荐使用}
\textbf{标准模式}在大多数实际应用中效果最好！
\end{alertblock}
\end{frame}

\begin{frame}{不确定性的分解}
\begin{center}
\begin{tikzpicture}[scale=1.2]
% 画一个大圆表示总不确定性
\draw[thick] (0,0) circle (2.5cm);
\node at (0,3) {\textbf{总的不确定性}};

% 两个重叠的圆
\draw[fill=blue!30, opacity=0.7] (-1,0) circle (1.2cm);
\draw[fill=red!30, opacity=0.7] (1,0) circle (1.2cm);

% 标签
\node[align=center] at (-1,0) {\textbf{认知}\\不确定性\\（可以减少）};
\node[align=center] at (1,0) {\textbf{环境}\\不确定性\\（无法避免）};

% 重叠区域标签
\node[align=center] at (0,-0.7) {\small 标准模式\\同时考虑两者};

% 举例说明
\node[align=left] at (-3.5,-2) {\small 认知：我们对个体了解不够};
\node[align=left] at (-3.5,-2.5) {\small 环境：测量误差、随机干扰};
\end{tikzpicture}
\end{center}

\begin{block}{核心思想}
承认存在两种不确定性，分别处理，效果更好！
\end{block}
\end{frame}

% 第四部分：性能有多好？
\section{因果引擎的性能有多好？}

\begin{frame}{抗噪音能力测试：回归任务}
\begin{columns}
\column{0.5\textwidth}
\begin{center}
\textbf{随着噪音增加，性能如何变化？}
\begin{tikzpicture}[scale=0.7]
\draw[->] (0,0) -- (5,0) node[right] {噪音水平};
\draw[->] (0,0) -- (0,4) node[above] {误差};

% 性能曲线（简化）
\draw[red, very thick] (0,0.5) -- (1,1.5) -- (2,2.5) -- (3,3.5) -- (4,3.8);
\draw[blue, very thick] (0,0.5) -- (1,1.2) -- (2,2.2) -- (3,3.2) -- (4,3.5);
\draw[green, ultra thick] (0,0.5) -- (1,0.7) -- (2,0.9) -- (3,1.2) -- (4,1.5);

% 图例
\node[red] at (4.5,3.6) {\tiny 传统方法};
\node[blue] at (4.5,3.3) {\tiny PyTorch};
\node[green] at (4.5,1.3) {\tiny 因果引擎};

% X轴标签
\node at (0,-0.3) {\tiny 0\%};
\node at (2,-0.3) {\tiny 30\%};
\node at (4,-0.3) {\tiny 50\%};
\end{tikzpicture}
\end{center}

\column{0.5\textwidth}
\textbf{30\%标签噪音下的表现：}
\begin{itemize}
    \item 传统MLP：误差 47.60
    \item PyTorch：误差 45.32
    \item \textcolor{green}{\textbf{因果引擎：误差 11.41}}
\end{itemize}

\vspace{1em}
\begin{alertblock}{惊人提升}
\textbf{性能提升76\%！}
\end{alertblock}
\end{columns}

\vspace{1em}
\begin{block}{关键发现}
噪音越多，因果引擎的优势越明显！
\end{block}
\end{frame}

\begin{frame}{抗噪音能力测试：分类任务}
\begin{columns}
\column{0.5\textwidth}
\begin{center}
\textbf{分类准确率对比}
\begin{tikzpicture}[scale=0.7]
\draw[->] (0,0) -- (5,0) node[right] {噪音水平};
\draw[->] (0,0) -- (0,4) node[above] {准确率};

% 准确率曲线（从高开始，略有下降）
\draw[red, very thick] (0,3.5) -- (1,3.3) -- (2,3.0) -- (3,2.8) -- (4,2.5);
\draw[blue, very thick] (0,3.5) -- (1,3.4) -- (2,3.1) -- (3,2.9) -- (4,2.6);
\draw[green, ultra thick] (0,3.8) -- (1,3.7) -- (2,3.6) -- (3,3.5) -- (4,3.3);

% 图例
\node[red] at (4.5,2.3) {\tiny 传统方法};
\node[blue] at (4.5,2.4) {\tiny PyTorch};
\node[green] at (4.5,3.1) {\tiny 因果引擎};

% 轴标签
\node at (0,-0.3) {\tiny 0\%};
\node at (2,-0.3) {\tiny 30\%};
\node at (4,-0.3) {\tiny 50\%};
\node at (-0.3,2) {\tiny 0.8};
\node at (-0.3,4) {\tiny 1.0};
\end{tikzpicture}
\end{center}

\column{0.5\textwidth}
\textbf{30\%标签噪音下的准确率：}
\begin{itemize}
    \item 传统MLP：88.50\%
    \item PyTorch：88.75\%
    \item \textcolor{green}{\textbf{因果引擎：92.25\%}}
\end{itemize}

\vspace{1em}
\begin{alertblock}{显著改善}
\textbf{错误率降低31\%！}
\end{alertblock}
\end{columns}

\vspace{1em}
\begin{block}{实际意义}
在现实的噪音数据中，因果引擎提供更可靠的预测
\end{block}
\end{frame}

% 第五部分：怎么使用？
\section{怎么使用因果引擎？}

\begin{frame}[fragile]{安装和基本使用}
\begin{block}{安装很简单}
\begin{lstlisting}
pip install causal-sklearn
\end{lstlisting}
\end{block}

\begin{block}{基本使用示例}
\begin{lstlisting}
from causal_sklearn import MLPCausalRegressor
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split

# 生成数据
X, y = make_regression(n_samples=1000, n_features=10, noise=20)
X_train, X_test, y_train, y_test = train_test_split(X, y)

# 创建和训练模型
model = MLPCausalRegressor(
    hidden_layer_sizes=(100, 50),
    inference_mode='standard',  # 推荐使用标准模式
    max_iter=200
)
model.fit(X_train, y_train)

# 预测和评估
score = model.score(X_test, y_test)
print(f"测试分数: {score:.4f}")
\end{lstlisting}
\end{block}
\end{frame}

\begin{frame}[fragile]{高级配置}
\begin{block}{自定义网络结构}
\begin{lstlisting}
model = MLPCausalRegressor(
    # 感知层配置
    perception_hidden_sizes=(128, 64),
    perception_activation='relu',
    perception_dropout=0.2,
    
    # 推理层配置
    abduction_hidden_sizes=(32,),
    
    # 推理模式
    inference_mode='standard',
    
    # 训练配置
    learning_rate_init=0.001,
    batch_size=32,
    early_stopping=True,        # 早停
    validation_fraction=0.2     # 验证集比例
)
\end{lstlisting}
\end{block}

\begin{alertblock}{性能建议}
对于有噪音的数据，使用 \texttt{inference\_mode='standard'} 效果最好
\end{alertblock}
\end{frame}

% 第六部分：理论基础
\section{理论基础（简单版）}

\begin{frame}{个体选择变量U的双重身份}
\begin{center}
\begin{tikzpicture}[scale=1.1]
% 画一个人
\node[circle, draw, fill=yellow!30, minimum size=1.5cm] (u) at (0,0) {U};

% 左边：个体选择器
\node[align=center] at (-3,1) {\textbf{作为个体标识}\\每个人都有独特的U\\像身份证号码};
\draw[->, thick] (-2,0.5) -- (-0.5,0.3);

% 右边：因果表示
\node[align=center] at (3,1) {\textbf{作为因果机制}\\连接输入和输出\\像基因影响身高};
\draw[->, thick] (0.5,0.3) -- (2,0.5);

% 下面：统一框架
\node[align=center] at (0,-2) {
\textbf{统一数学框架}\\
$P(\text{输出}|\text{输入}) = \int P(\text{输出}|U) \cdot P(U|\text{输入}) \, dU$
};

% 解释
\node[align=left] at (-4,-3.2) {\small $P(U|\text{输入})$：从观察推断原因};
\node[align=left] at (1,-3.2) {\small $P(\text{输出}|U)$：从原因预测结果};
\end{tikzpicture}
\end{center}

\begin{block}{简单理解}
U既是每个人的"身份证"，又是连接原因和结果的"桥梁"
\end{block}
\end{frame}

\begin{frame}{为什么这种方法有效？}
\begin{enumerate}
    \item \textbf{正确的思维方式}
    \begin{itemize}
        \item 个体差异是信息，不是噪音
        \item 学习通用规律，而不是死记硬背
    \end{itemize}
    
    \item \textbf{量化不确定性}
    \begin{itemize}
        \item 明确区分"不知道"和"随机性"
        \item 让模型知道自己"不知道什么"
    \end{itemize}
    
    \item \textbf{数学优雅}
    \begin{itemize}
        \item 柯西分布的线性稳定性
        \item 可以直接计算，不需要复杂的采样
    \end{itemize}
    
    \item \textbf{实际有效}
    \begin{itemize}
        \item 在多个真实数据集上验证
        \item 在噪音环境中显著优于传统方法
    \end{itemize}
\end{enumerate}
\end{frame}

% 第七部分：应用场景
\section{适用场景}

\begin{frame}{什么时候用因果引擎？}
\begin{columns}
\column{0.5\textwidth}
\textbf{特别适合的场景：}
\begin{itemize}
    \item $\checkmark$ 数据有很多噪音的时候
    \item $\checkmark$ 需要理解个体差异
    \item $\checkmark$ 医疗诊断（个性化）
    \item $\checkmark$ 金融风险评估
    \item $\checkmark$ 推荐系统
    \item $\checkmark$ 异常检测
\end{itemize}

\column{0.5\textwidth}
\textbf{优势不明显的场景：}
\begin{itemize}
    \item $\times$ 数据非常干净
    \item $\times$ 纯图像分类
    \item $\times$ 不需要解释性的场景
    \item $\times$ 计算资源极其有限
\end{itemize}
\end{columns}

\vspace{1em}
\begin{block}{经验法则}
当数据质量不确定或需要鲁棒性时，因果引擎是理想选择
\end{block}
\end{frame}

\begin{frame}{真实数据集上的表现}
\begin{table}
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{数据集} & \textbf{传统MLP} & \textbf{因果引擎} & \textbf{提升} \\
\hline
加州房价 & 0.65 & 0.78 & +20\% \\
葡萄酒质量 & 0.55 & 0.71 & +29\% \\
波士顿房价 & 0.62 & 0.74 & +19\% \\
糖尿病 & 0.41 & 0.52 & +27\% \\
\hline
\end{tabular}
\end{table}
{\footnotesize *在20\%标签噪音下的R²分数}

\vspace{1em}
\begin{alertblock}{关键洞察}
即使在中等噪音水平下，因果引擎也能提供显著的性能改善
\end{alertblock}
\end{frame}

% 第八部分：总结
\section{总结与展望}

\begin{frame}{核心贡献}
\begin{enumerate}
    \item \textbf{新的机器学习范式}
    \begin{itemize}
        \item 从学习相关性到学习因果关系
        \item 把个体差异当作特征，而不是噪音
    \end{itemize}
    
    \item \textbf{实用的实现}
    \begin{itemize}
        \item 完全兼容scikit-learn API
        \item 高效的分析计算
        \item 容易集成到现有工作流程
    \end{itemize}
    
    \item \textbf{出色的鲁棒性}
    \begin{itemize}
        \item 在噪音环境中性能优异
        \item 适合现实世界的混乱数据
    \end{itemize}
\end{enumerate}

\vspace{1em}
\begin{block}{一句话总结}
因果引擎通过理解"为什么"而不仅仅是"是什么"，为机器学习带来新的可能性
\end{block}
\end{frame}

\begin{frame}{未来发展方向}
\begin{itemize}
    \item \textbf{理论扩展}
    \begin{itemize}
        \item 扩展到其他概率分布
        \item 多任务因果学习
        \item 时间序列因果推断
    \end{itemize}
    
    \item \textbf{应用拓展}
    \begin{itemize}
        \item 大规模数据集优化
        \item 与深度学习架构集成
        \item 领域特定定制
    \end{itemize}
    
    \item \textbf{工具生态}
    \begin{itemize}
        \item 可视化工具
        \item 自动超参数调优
        \item 云部署支持
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\begin{center}
{\Huge \textbf{谢谢大家！}}

\vspace{2em}

{\Large 有问题可以讨论}

\vspace{2em}

{\large 让机器学习变得更智能 }
\end{center}
\end{frame}

\end{document}
